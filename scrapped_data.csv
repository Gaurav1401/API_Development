,Header,DateTime,Article_Url,Content
0,SAP’s key to generational change is its best-kept secret,"May 12, 2022 7:00 AM",https://venturebeat.com/2022/05/12/saps-key-to-generation-change-is-its-best-kept-secret/,"SAP is no stranger to generation change. Turning 50 years old this year, it wasn’t until SAP reached middle age that it grabbed its original claim to fame as the leader in enterprise software applications. In the runup to Y2K, enterprises looked to a middle-aged software provider that was (it predated SAS by four years) to represent a new generation of enterprise business system. In the past few years, SAP’s top management underwent a generational change with much of the current executive team populated by a bunch of forty year-olds. But generation change goes beyond showing off young faces. It’s the inconvenient truth that cloud and the disruption caused by the pandemic have pushed SAP and its customers into yet another transformation wave whether they like it or not. But if you ask most SAP customers as to what that generational change is, in all likelihood they will point to S/4HANA, which is the successor to SAP ECC and Business Suite. Or they might think of SAP’s various cloud offerings for HANA, data warehousing, and analytics. Few will guess about the piece that is represents its true underbelly of change: SAP’s Business Technology Platform (BTP). SAP talks BTP, but the message is still muddled Call it SAP’s best-kept secret. Ever since SAP introduced it when it rebranded from SAP Cloud Platform roughly a year ago, we’ve been confused as to what BTP is. And it’s not for lack of content, promotion, or verbiage. There’s the requisite technology stack chart showing all the latest APIs, data sources, and programming languages. There are web pages devoted to BTP, but they pinpoint what BTP can do rather than what it is. Addressing application development, integration, automation, AI, and planning & analysis, BTP offers nearly a hundred prebuilt cloud services covering the waterfront. It includes a “Discovery Center” where customers can assess business outcome missions from the perspective of the implementer. And with roughly a third of the portfolio offered on a freemium basis, SAP is certainly serious about getting BTP services in developer’s hands. Other content on the website shows selected customer success stories, such as Farys, a Belgian wastewater utility, that was able to get reports 10x faster and build a customer self-service portal that automated over 90% of interactions, all with BTP. But nowhere does SAP clearly put its cards on the table and flatly state what BTP actually is. Spoiler Alert! BTP’s about SAP app modernization At SAP’s main SAPPHIRE event this week, we finally got an explanation. BTP is the technology foundation that changes the way companies develop and extend their SAP applications. Part of the confusion is that a key part of the message behind BTP is not new. From the days of S/3 and up through ECC, SAP warned customers not to customize inside the application. However, with APIs still another 10 – 15 years off in the future, customers would have had to custom-build their own interfaces to extend SAP to keep the core application code pristine. But SAP also provided tools for customers to take the law into their own hands, with the ABAP language, later supplemented by Java. And so typically, most ECC implementations are likely to have a lot of custom ABAP code inside them, especially in financial accounting modules. And therein lies the rub. With all that intrinsic custom code, two words ended up striking terror into the hearts of ECC customers: Version Upgrade. Updating versions or instituting patches inevitably meant checking countless interfaces to ensure that functionality would not be broken. Some of SAP’s largest customers had up to tens of thousands of customizations in their own implementations. APIs to the rescue By the time SAP introduced what is now known as BTP, APIs became established practice and popular with developers. The guiding notion behind BTP is to abstract all the customizations outside the core code through APIs. Reliance on APIs is a rule of thumb for cloud SaaS applications, which would otherwise constantly break if they let their customers mess with the innards. With BTP, SAP is telling its customers that they should adopt these practices, regardless of what generation SAP software they are running, or regardless of whether they are running in the cloud (or not). The theory is that if you keep the APIs stable, the core application should be well protected and the extensions should interoperate with them. While APIs are generally associated with integrating applications or connecting applications to different data sources, they could also be used for the purpose of protecting the core code. And while BTP is often described as a collection of reusable services, at its heart, BTP provides APIs enabling SAP customers to wean themselves off years of bad habits, keeping modifications away from the SAP app. Given that APIs are the default mode of interoperating with SaaS applications, it’s not surprising that BTP initially surfaced as part of the rollout of what used to be known as SAP Cloud Platform. Let’s cut to the chase SAP needs to clarify what BTP actually is. It’s more than just a bunch of APIs and services. At the end of the day, BTP is really about modernizing how classic and low-code developers work with SAP applications. And we’re not just talking about the current generation like S/4HANA. Nonetheless, most SAP customers adopting BTP are likely to be headed for newer offerings like S/4HANA, Data Warehouse Cloud, or Analytics Cloud, but there is also an important bridge play for customers who may still need to have some assets on ECC. And in fact, it has also been back-ported to legacy SAP solutions such as Business ByDesign. BTP is also about SAP broadening outside the ABAP developer base, and for that matter, also reaching out to citizen developers. With BTP, they can write apps or changes in the language of their choice, which is pretty critical given that ABAP is a legacy language that is not attracting many developers these days (akin to COBOL). And that extends from traditional coding to low code/no code tools that SAP is starting to promote. It’s a portfolio of prebuilt cloud-based services for common capabilities to give developers jumpstarts that can be run against SAP (and for popular sources, non-SAP) data in the cloud or back on-premises. For SAP and its customers, BTP represents both challenge and opportunity. It’s an opportunity to make customer SAP implementations less brittle, and enable customers to become more agile by overcoming the fear, not only of upgrades, but of adding new functionality. During the course of the pandemic, businesses have had to embrace change, from hybrid workplaces to accelerating digital processes, new approaches to resiliency, and more recently, paying more attention to sustainability. Upgrades the old fashioned way just won’t keep pace. But it’s also challenge, because organizations need to revisit their development practices, and in some cases, rip apart and refactor existing hard-coded modifications. At SAPPHIRE this week, we heard about the account of one large SAP customer with over 50,000 hard-coded modifications to its classic applications make the full migration to BTP in just a year (yes, SAP does have an extensive modification checker, but we digress). We expect that for most the process will be a longer haul. Progress ain’t easy, but since we’re on the topic, we have one selfish request for SAP: Could you please use BTP to move Ariba into the 21st century?"
1,"ThoughtSpot adds new BI capabilities, editions for smaller organizations","May 11, 2022 2:41 PM",https://venturebeat.com/2022/05/11/thoughtspot-adds-new-bi-capabilities-editions-for-smaller-organizations/,"At its Beyond 2022 conference yesterday, independent business intelligence (BI) player ThoughtSpot announced the salient points of its revamped Modern Analytics Cloud platform, including new capabilities and new editions available for small teams, medium-sized entities and large enterprise organizations. ThoughtSpot’s Series F fundraising round, back in November, garnered $100M and a $4.2 billion valuation for the company. The valuation reset accompanied a change in business model, too. While the platform premiered on the market with a monolithic, on-premises natural language-focused business intelligence platform and a six-figure price tag, ThoughtSpot completely switched gears, moving to a fully-SaaS model. The technology has changed along with the deployment model. While the original ThoughtSpot platform required all data to be ingested into, and modeled within, its own storage platform, it now leverages major data warehouse and lakehouse platforms — including Amazon Redshift, Snowflake, Databricks, Google BigQuery, Starburst, Dremio and Microsoft’s Azure Synapse Analytics — for the actual storage of data. Essentially, ThoughtSpot is now implemented as an analytics engine, with its own modeling language, and no longer seeks to be the physical repository for the data. This avoids lengthy data movement and inefficient, risky data duplication, taking a customer-driven approach rather than a vendor-centric one. Yesterday’s announcement rang in further changes to the pricing model, making ThoughtSpot’s analytics platform available in three editions, which differ in data volume/capacity limits, but which impose no restrictions on number of users. A Team Edition is available for $95/month, with a data volume limit of 5 million rows. While there is no limit in the number of users, there is a limit of one group of users, making the Team edition a departmental solution, with appropriate nomenclature. Team Edition offers unlimited queries, and support is community-based. Pro Edition starts at $2500/month, with a limit of 100 million rows — increasing both the price and the data capacity by about 20x — although the rows:dollar ratio actually decreases a bit. For Pro, the number of user groups increases from one to five, making it a solution more appropriate for small and medium-sized organizations or as a divisional solution for larger ones. 24/7 direct support by ThoughtSpot is part of the package, with certain service-level agreements (SLAs). The actual monthly billable amount for Pro will vary by query activity; however, startups, nonprofits and educational institutions, with less than 100 people and under $10 million in annual revenue, are eligible for a special variant of Pro that eliminates per-query charges. The top-of-the-line solution is Enterprise Edition, which eliminates caps on data volume and number of user groups. Here too, pricing is based on actual queries, and capabilities include higher-grade SLAs, enhanced data encryption, support for AWS PrivateLink/Azure Private Link, single sign on (SSO) and VPN support. In addition to the new editions and pricing, ThoughtSpot announced several new core capabilities. These include new “CodeSpot” searchable repository of open-source ThoughtSpot blocks and code samples; ELT Live Analytics templates (custom ELT jobs built to work with Matillion); new third-party data blocks; integration with dbt Labs‘ SQL-based data pipeline platform, and new SpotApps, with templates for ServiceNow, Snowflake, HubSpot, Okta, Google Analytics, Google Ads, Jira, Redshift and Databricks. Also announced were ThoughtSpot Sync, which can trigger actions in other applications and services through APIs; Bring Your Own Charts , which lets customers bring visualizations from javascript or d3 libraries directly into ThoughtSpot’s Live Analytics interface; and Monitor, an automated KPI observation and alerting facility. For my own purposes, I like to analyze new offerings relative to others in the market, both to determine value, but also to observe industry trends. The availability of three pricing tiers for ThoughtSpot’s platform, as well as its cloud orientation, begs some comparison to Microsoft’s Power BI. The latter offers three major tiers as well: Free, Pro and Premium, with the last of these starting at $4995/month and aimed at enterprises, much like ThoughtSpot’s Enterprise Edition. There are key differences, though. While Power BI Premium doesn’t limit the number of consumption-only users, it does have additional per-seat pricing for users who need authoring capabilities. On the other hand, it offers dedicated infrastructure and doesn’t have any usage-based fees. Of course, the higher the usage, the more compute capacity a customer may want, which would mean adding dedicated infrastructure nodes, with a commensurate increase in monthly pricing. One way or another, you get what you pay for, or vice versa. Meanwhile, Power BI lets users import data in their BI models or leave it in the source system. It also provides for so-called compositie models, where data storage for a single BI model can be split between the local and remote. The point here, though, isn’t to measure parity between ThoughtSpot and other BI platforms, but rather to discern some trends of consensus in the market. What we can see overall, is that business intelligence, which has been around since the 1990s, maintains its core tenets of slice-and-dice analytics but has modernized with the sea changes in database technology and computing overall. Today, it’s all about the cloud, integrating with other platforms in the ecosystem, and leveraging data from a variety of sources, without requiring the data to be moved. The barriers to entry for BI have been lowered, with simplified getting-started experiences, and very accessible pricing for smaller organizations. Large organizations will still pay handsomely, but will ostensibly see comprably handsome ROI, in terms operational efficiencies and competitive differentiation. The doctrine of data-driven operation and digital transformation is enabled by BI, which needs to be low-friction and accessible at the low-end, while facilitating robust rewards, usually accompanied by equally robust pricing, at the high-end. ThoughtSpot and its platform have changed immensely since the early days, as has the BI space, with so many players having been acquired in the last few years. ThoughtSpot is now well-aligned with industry trends and seems driven by them. If the remaining independents like ThoughtSpot are to succeed, they’ll need to conform to these trends and even get a bit ahead of them. Some will do well there; others less so. ThoughtSpot is clearly all-in on retooling and revamping for today’s analytics workloads, despite the business intelligence market’s evolution into a very crowded, competitive space."
2,"Google announces AlloyDB, a faster, hosted version of PostgreSQL","May 11, 2022 12:15 PM",https://venturebeat.com/2022/05/11/google-announces-alloydb-a-faster-hosted-version-of-postgresql/,"Data-laden users will have a new option for storing their information in the cloud now. Google Cloud Platform (GCP) today announced a new database option called AlloyDB that’s built around the PostgreSQL open-source database that has been a popular choice for developers for more than 30 years.  The new database is designed to appeal to users with a code stack that relies upon a full-featured database offering options like atomicity, consistency, isolation and durability (ACID)-compliant transactions, stored procedures or triggers. Google’s team believes that it will compete directly with legacy offerings from companies like Oracle, IBM or Microsoft by delivering the classic features in a modern, cloud-native package.  “We’ve got lots of customers, like travel agencies, retailers, auto manufacturers, or financial services who bought these very expensive, proprietary databases and are trying to really break free from them and move on to open source.” explained Andi Gutmans, the general manager and vice president of databases at Google Cloud. The PostgreSQL platform is a popular option because of its strong performance, a broad set of features and a large community of developers. The open-source license is attractive because users feel less locked into one company. If the contract terms or the price grows too onerous, they can move to another service provider supporting the product or build up a team in-house.  “You can kind of guess why customers want to move,” said Gutmans. “The cost is definitely one part, but there are a lot of prohibitive licensing terms. They have audits being done against them. There are a lot of, I would say, unfriendly practices.”  Google’s new version will be hosted in their cloud and priced as a service. The new pricing model is designed to be simpler and free from the kind of hidden charges that often create large and unexpected bills. Their model, for example, won’t charge for I/O, a common extra in some other contracts from cloud providers like Amazon Web Services (AWS).  The new offering from Google is joining several other companies that are building database products around PostgreSQL. Some cloud providers like DigitalOcean, Vultr and AWS are rolling out managed versions of popular open-source databases like MySQL, Redis and PostgreSQL. These products deliver the stock version of the database while handling many of the chores of installing the software, configuring the server and keeping it up-to-date as new security patches appear.  Meanwhile, other companies are building more elaborate versions around the open-source database, while adding some new features that allow them to create a new brand. Companies like Yugabyte and Fly.io are creating versions of PostgreSQL that scale to support large datasets distributed around the world. They manage many of the chores of synchronizing the data between different instances and shards. Some like Oracle and PlanetScale are doing something similar with MySQL, another popular open-source option.  Google plans to distinguish itself with faster performance and a rock-solid service-level agreement. They’ve rewritten some core storage routines to speed up both transactional and analytical queries. Their initial internal benchmarks suggest that their version will be four times faster than stock PostgreSQL and twice as fast as Amazon’s Aurora, another competitor following a similar path.  They’ve also included a columnar accelerator that stores the dataset in columns, an approach that can speed up complex searches and queries. Analytical tasks like creating reports or watching for important anomalies often run more efficiently in environments where the data is stored in columns, while transactions can be faster when the data is stored in rows. AlloyDB will offer users the ability to configure the storage to support their pattern of usage.  “Customers want to analyze the data in real time in order to make additional decisions very, very quickly about things like personalization or fraud detection or so forth,” explained Gutmans. “What we’ve actually added Is an analytical capability, so now you have a hybrid transactional and analytical system. They can run analytics up to 100 times faster than [stock] open-source PostgreSQL.” Google also integrated the database with Vertex AI, one of its options for building and deploying machine learning models. Developers will be able to work directly with these models with queries and stored procedures.  PostgreSQL is also popular because the community has created a number of extensions that add features for particular applications. Mapmakers and developers working with location data, for example, rely upon PostGIS, a version optimized for storing and searching collections of points specified by latitudes and longitudes.  “[Compatibility] was actually a core design principle for us.” said Gutmans. “This is why we decided to take PostgreSQL and extend it as opposed to building a PostgreSQL compatible system. We have a PostgreSQL API on spanner, right? But you can’t build that to be 100% compatible, and so what we’ve done here is we’ve stayed true to PostgreSQL.”  Gutmans estimates that AlloyDB will begin with support for more than 50 of the most popular extensions and add new ones following customer demand.  “AlloyDB is fully compatible with PostgreSQL and can transparently extend column-oriented processing.” said Takuya Ogawa, a lead product engineer at Plaid who is testing a pre-released version of Google’s revamped PostgreSQL.  “We think it’s a new, powerful option with a unique technical approach that enables system designs to integrate isolated OLTP, OLAP and HTAP workloads with minimal investment in new expertise.”  Others agreed and emphasized the combination of full compatibility with cloud availability.  “AlloyDB provides us with a compelling relational database option with full PostgreSQL compatibility, great performance, availability and cloud integration. We are really excited to co-innovate with Google and can now benefit from enterprise-grade features while cost-effectively modernizing from legacy, proprietary databases.” said Bala Natarajan, senior director of data infrastructure and cloud engineering at PayPal. Some execs are attracted to the support and management that Google offers. The sales literature emphasizes the service’s ability to handle many of the scaling, back up and replication tasks. Google will deploy machine learning-based models to learn from users and adapt to their query patterns.  “With AlloyDB, we have significantly increased throughput, with no application changes to our PostgreSQL workloads. And since it’s a managed service, our teams can spend less time on database operations and more time on value-added tasks,” said Sofian Hadiwijaya, CTO and cofounder of Warung Pintar. Google believes that customers like this will form the basis for a strong customer base. They need compatibility with their current legacy options at a lower price with open-source software’s flexibility.  “The predictions are that 70% of new in-house applications will be developed on open source and 50% of existing proprietary databases will either have migrated to open source or begin the process of converting,” said Gutmans. “This is just something we’ve been hearing time and time again from our customers.”"
3,Helping nontechnical execs select analytics solutions,"May 4, 2022 7:40 AM",https://venturebeat.com/2022/05/04/helping-non-technical-execs-select-analytics-solutions/,"Every company seeks to make better decisions driven by data, analytics and relevant context. To support these decisions, companies must make investments that allow employees to use and experiment with a range  of available data sources and vendors in a timely manner without being locked into any potential evolutionary dead end. As technology continues to advance and employees now expect the scale of cloud computing, performance to support real-time analysis and access to relevant documents and media as part of their analytic environment.  >> Read more in the Data Pipeline << These demands force companies to upgrade their data environments over time in order to maintain a competitive advantage, as well as to avoid having to explain to investors that their current data and analytics capabilities are disadvantaged compared to the market at large.  Understandably, analytic investments are one of the most substantial technology investments a company can make, which leads to the participation of a variety of departments in this purchasing process. It is not uncommon to see non-technical executives and departments participate in some aspects of selecting an analytic solution. Yet, it can be difficult at times for executives lacking the technical expertise to both get the information they need and to ask relevant questions to vendors seeking to upgrade an organization’s analytic capabilities.  In the enterprise, there are several departments that are increasingly involved in analytics purchases. Their key concerns generally relate to the selection of a new analytics solution, including the procurement, finance, revenue and operations surrounding the newly selected tool. Here’s a look at why it matters: The procurement department will always be involved in significant technology purchases, as formal purchasing processes are necessary at any large enterprise. In looking at an analytics solution, procurement departments seek to show their value in controlling costs. This means providing contractual discounts or the ability to trigger discounts based on some sort of usage or business activity.  Procurement will also want to be able to show how a solution is either superior to all other solutions or to clearly show that a solution meets all listed criteria. This includes defining key performance indicators (KPIs) or management by objective (MBO) metrics used to define success and ensuring that the solution’s success is aligned to business success.  Enterprise data needs to make requests around performance, scale and reliability. This can be difficult to concurrently support without bringing multiple best-in-breed solutions, refactoring legacy data solutions, or by making compromises in the flexibility and variety of use cases that can be supported. Frankly, vendors can sometimes help with this process by adding criteria for future facing demands that they are better positioned to support compared to other vendors, but this approach requires alignment between the vendors and stated client needs. For a procurement department that is considering analytic investments: Align any new and significant analytics and database contracts (as well as any other significant software and data investments that add new data sources to the enterprise) to existing business intelligence and key software contracts. This way, new data management capabilities will support existing data, analytics and machine learning technologies. This becomes increasingly complicated as the typical billion-dollar revenue business now supports over 900 applications over its network. Additionally, be sure to provide MBOs to analytics vendors to determine how analytic performance and outputs can be aligned to the business.  The finance and accounting departments will always look at analytics in terms of cost. However, this limited perspective ignores the elevated state that the CFO now has within the business. In the majority of businesses, the CFO is treated as a top-two or top-three executive based on their visibility to the top and bottom lines and the role of managing cash as a core strategic role.  This means that the value of analytics for the CFO goes far beyond the pure cost of the solution as real-time analytics provides the CFO with the ability to potentially close the books, support public and investor reporting demands and support strategic forecasting scenarios across multiple entities, countries and currencies. In speaking with finance executives, the role of analytics in supporting strategic business perspectives across sales, marketing, supply chain, operations, talent and succession planning, treasury, intercompany consolidation and investor relations will be top of mind. In addition, analytics solutions must still provide guidance in the language of business: capital expenditures (CapEx), operational expenditures (OpEx), total cost of ownership (TCO), return on investment (ROI), payback period, internal rate of return (IRR) and the potential predictability of cost and return. These metrics all matter because they provide a consistent standard for comparing all projects across technology, operations, revenue, human resources and other departments based on expected financial impact. Some of these metrics are dependent on the delivery of the project, such as OpEx vs. CapEx. Value-based metrics are often dependent on the believability of the value being proposed or the organization’s ability to execute on the value being stated.  For finance departments considering analytic investments: Look at how an analytics solution will help support strategic views of data, including real-time support for the insights that drive board decisions, product investments and revenue improvement. Avoid vendor lockin and include a “wish-list” of metrics that would accelerate the organization’s ability to make big decisions, such as business unit creation, expansion, or retirement.  From a more tactical perspective, look at the value of analytics based on projects that can be practically deliverable within a two-to-three year time based on current or readily available skills and resources. Include at least one quick win that can be accomplished within the first year that provides meaningful contribution to the ROI of the project. And, putting on the strategy hat, finance professionals should also consider the need for managing compliance and governance for all data under a single platform or control plane to avoid the inevitable complexities of audits, compliance, governance, lineage and unit-based economics for digital applications and services. Third, revenue-driving departments will always be interested in the role of analytics to help qualify and close sales. The role of analytics in helping to quantify the potential revenue associated with known potential customers is well documented, but sales and marketing departments are aware that the majority of a buyer’s journey occurs before a potential buyer ever speaks with a sales representative. With this in mind, it is increasingly important for revenue-supporting departments to gain visibility to non-sales interactions across marketing, service and other line-of-business departments to understand how contacts are either interacting or are not following up with the company. From a practical perspective, this means that sales and marketing stakeholders need to ensure that any new data investments support all the data that helps support campaigns and sales processes, including relevant personalization, automation, environmental, economic and cyclical topics that can potentially affect the ability and willingness to purchase.  Recommendations for revenue-based departments considering analytics departments: Don’t settle for basic visibility to existing customer relationship management (CRM) and marketing campaign data from an analytics perspective. Consider the key issues that come into play in slowing down or abandoning sales, including weather, publicly reported financial records, relevant government policy, training issues associated with email and call quality and frequency and other environmental and ecosystem concerns.  This means potentially analyzing a variety of activities, documents, calls, conferences, videos and other requests associated with marketing and sales. The goal should be to provide guidance and forecasting that lines up with regular sales meetings to help the revenue team at all times, not just to support formal reports at the end of the month or the end of the quarter. From a service perspective, this ability to provide a common and shared version of data-driven truth allows companies to see the customer journey and propensity to buy from initial contact to ongoing support. Finally, operations, supply chain and logistics departments should also make sure that they are included in analytics selection processes, especially as the supply chain is now top of mind in the business world in light of geopolitical stresses and resulting shortages. This is an opportunity to translate manually tracked metrics across plants, remote offices and field locations into more automated methods for data collection and analysis.  However, to fully capture the context associated with manually collected data, analytic solutions may have to collect time-series, geolocation, connected graph and other non-standard data. This requires supporting analytic access to large volumes of data to create appropriate reports and to provide guidance to all stakeholders. In addition, by digitizing this data, companies may gain additional insights by being able to combine operational data that was previously either siloed or off-line with more traditional enterprise applications.  Recommendations for operational departments considering new analytic solutions: The operational needs for data include a wide variety of formats which are often outside the visibility of the stakeholders that most typically considered as analytic stakeholders: IT, data, finance and sales. Make sure that your needs for highly available and high-performance analytics, especially for location and time-based analysis, are supported by any new analytics investment. In the 2020s where cloud computing is readily available and Moore’s Law continues to make computing more available, it should no longer be necessary to wait for hours to transform data or run a query to get the answer you are looking for, no matter how complicated it is. Life is short and computing is cheap: be demanding with your new analytics solutions. In addition, make sure that data-driven insights can be supported anywhere based on the computing and end-user interfaces available across all working locations.  Across all of these areas, business stakeholders consistently see the need to support distributed and varied data sources and face the potential for having to support data that may be stored in a public cloud, private cloud, internally hosted server, or even specific edge computing or end-user devices.  Businesses should look for data architectures that explicitly support providing access to distributed data sources across a hybrid cloud environment on platforms that don’t care what your preferred technical infrastructure or preferred cloud vendor may be. Companies need a distributed data platform that is complex enough to handle any data, any source, any computing platform to provide the simplicity of having business users choose to access what they need whenever they need it. This concept is currently being described as a distributed data cloud, which has the following qualities: Business stakeholders pulled into a discussion regarding analytic solutions may find themselves overwhelmed by the technical jargon that inevitably is needed to describe the technologies involved. My hope is that the guidance provided in this blog will help business users to stay grounded, be better equipped to analyze solutions based on their particular expertise and select analytics solutions while providing value to the business at large."
4,Data technology comes to the construction industry,"May 3, 2022 11:40 AM",https://venturebeat.com/2022/05/03/data-technology-comes-to-the-construction-industry/,"It’s no secret that data is changing the world as we know it. Like every other industry, more data from more sources are coming to architecture, engineering and construction (AEC). >> Read more in the Data Pipeline << Toric is using data and analytics to transform the AEC industry. They provide real-time insights to help AEC firms and owners, as well as operators, reduce waste and increase sustainability. This data platform enables construction professionals to make better, data-driven decisions at a much faster pace than what has previously been possible. I worked in the construction industry from 2007 to 2010. Then, only a few of the most forward-thinking firms were using data and technology to improve business processes. Over the past 10 years, AEC firms hired more people who grew up with technology and were comfortable using technology in their jobs. COVID-19 has further awakened a sleeping giant. Leading AEC firms and their project managers have realized the critical need to capture data from job sites remotely. Every construction firm is at a different place in its use of data and digitalization. The use of ERP, scheduling, project management, BIM, drones, scans and photography for open space varies by company. More data and technology solutions for the industry are being introduced every year. Procore led the way two decades ago. Today, Turner Construction is using robots to handle dangerous tasks on job sites. Most of the AEC companies using data today are doing so with old data. When data is collected it needs to be cleaned, structured and analyzed to improve safety, quality, productivity and profitability. Data value and accuracy decrease over time. For instance, a photo of an active job site on Monday will no longer be accurate on Tuesday. Ad-hoc processes result in a number of challenges for AEC firms: Toric is at the forefront of this data-driven transformation. The company is working to address the aforementioned challenges by providing real-time data for real-time decision-making to reduce errors, mistakes, costs and risks. The data landscape is chaotic. There is more data, more sources, tools and solutions including artificial intelligence/machine learning, data mining, data science, predictive and prescriptive analytics, et al. Toric has created a no-code data platform to take advantage of all these tools. The platform integrates, transforms, visualizes and automates data across projects. Data is then consolidated in one workspace for analysis. They have more than 100 tools available to clean, transform and augment data. Additionally, the data is updated in real-time, so project managers can make immediate, well-informed decisions to properly equip the project to move forward. The construction analytics platform helps deliver more accurate bids, tracks progress and improves digital delivery by referencing all past project data. It integrates Procore, Autodesk, ERP systems and spreadsheets. Estimation and project tracking are all analytics-driven. Historical data is leveraged for data applications. Architecture and engineering firms can build an analytics model for their BIM design process. They do this by tapping into BIM models and other data sources to support data-driven design, QA, quantification and change management. Users can perform continuous data modeling, track design to project requirements and create data apps to improve customer experience. Owners and operators use Toric to track key building metrics during design, integrate and compare bids against design and create a complete data lifecycle for digital twins. The average AEC firm with 100 projects is adding 1PB of data every year. Much of this data is unstructured. It’s expensive and hard to find data analysts and scientists to get value from the data in addition to capturing, cleaning and integrating it. Suffolk Construction is a 40-year-old, $3.9 billion firm based in Boston. It’s one of the most mature firms regarding its data strategy, with 30 data scientists on staff. Suffolk has integrated three of its 20 systems with Toric’s platform, replacing their home-grown systems for data ingestion and data capture. HITT, a 2,000-person, construction management firm founded in 1937 has one data analyst on staff. With Toric, their data analyst will be able to automate thousands of projects using just one tool. Data quality, data cataloging and real-time data analysis for the AEC industry did not exist three years ago. Advances in the industry will see data leaders making significant impacts on several fronts. Environmental issues are a key issue today and can only be addressed with data. More sophisticated and precise proposals will result in reduced costs and a stronger competitive position as data tools are used to evaluate and project costs. Conscientious owner/operators who own a lot of real estate and care about the health and environmental compatibility of their properties. They’re requiring designers, engineers and contractors to know the amount of carbon that goes into constructing a new building. Embodied carbon is a major issue for owners that want to be environmentally conscientious. They want to see the analysis of the carbon footprint and to know if it’s more efficient to build their new or reclaimed building with steel or concrete. They want to know the environmental impact of a design change. Previously, information about embodied carbon was subjective. Today, it’s objective and it’s incumbent for AEC firms to have a firm grasp on their data to be competitive."
5,AWS sharpens focus on modern data strategies with an array of new products,"April 22, 2022 6:00 AM",https://venturebeat.com/2022/04/22/aws-sharpens-focus-on-modern-data-strategies-with-an-array-of-new-products/,"Amazon Web Services (AWS) is on a mission to enable organizations — including startups, enterprises and government agencies — to become more agile and innovate faster at lower costs. Last year, an AWS executive told VentureBeat that a priority for AWS in 2022 will be automation at scale, allowing customers to bolster the security of their cloud environments. To advance its mission, AWS today unveiled innovations across several services, including databases, machine learning, IoT and application development. The announcement came yesterday at the AWS Summit, San Francisco, where Swami Sivasubramanian, VP of data, analytics and ML services at AWS, offered details on the new product offerings in a keynote address. While AWS has seen massive adoption, according to Sivasubramanian, it’s just getting started. His 90-minute keynote highlighted AWS’ plans to expand its capabilities and services across more global regions and to scale up the pace of innovation for customers. Sivasubramanian said analysts estimate that 5% to 15% of IT spending has moved to the cloud — a sign that many more workloads will migrate to the cloud in the coming years. He noted that this meant there’s a need for more innovation. While the primary benefits of agility, cost and elasticity were major reasons many customers chose AWS, Sivasubramanian said AWS’ capability to speed up the pace of innovation was another key benefit. With an enormous vibrant partner ecosystem of more than 100,000 partners, he said AWS is constantly thinking of ways to invent on behalf of its customers. “In 2021 alone, we added 3,084 services and significant features. That is 3,084 more capabilities that our customers are equipped with to address the needs of today and the future. Since the challenges for a customer today are often different from the ones of tomorrow, it makes it all the more important to pick the cloud provider that will be the best technology partner now but also in the future,” he said. Sivasubramanian said effective leaders use the immense amount of data that is available to them to make informed decisions, look around the corners and take meaningful action. He said such organizations build a modern data strategy to deliver insights to the people and applications that need it. According to Forrester, organizations that take a data-driven approach to decision-making grow more than 30% annually. However, most organizations aren’t able to put this wealth of data to work. A recent Accenture study revealed 68% of companies are not able to realize tangible and valuable benefits from the data. Sivasubramanian said that’s the case because it’s not easy for organizations to instantly turn on a switch and become data-driven. “It requires leaders to truly embrace data-driven decision making and set the expectation that decisions need to be anchored in data. They also need to lead by example, and make data driven decision making. And there are also technical challenges when it comes to setting up access control and scaling your data infrastructure,” he said  Sivasubramanian added that these factors can slow down organizations that are trying to put the data to work to solve these challenges. He added that modern data strategy needs to be scalable, flexible, able to address various use cases and also support the projects of tomorrow. It also has to consider things like governance, according to Sivasubramanian, ensuring that the right people have access to the right data. It must be available when it’s needed, in a cost effective manner. “Your infrastructure should serve the needs of the business data at every stage of its journey — from databases and data lakes, to putting it to work through analytics and machine learning with the right governance and controls embedded throughout. Our customers are using AWS tools and services to become data-driven organizations and make faster data decisions.” Sivasubramanian said a typical modern data strategy comprises three pillars: modernize, unify and innovate. He noted that customers start modernizing that infrastructure by first migrating to the cloud and moving towards infrastructure that enables them to achieve the scale they need at the right costs while reducing the operational burdens. Then they unify the data storage and access control infrastructure. Finally, they put the data to work by leveraging analytics and ML on top of the data. “Modernizing your data infrastructure allows you to remove costly, undifferentiated heavy lifting, such as database provisioning, patching and continuous backups, failover maintenance, software licensing and more.” This is where AWS comes in. According to Sivasubramanian, AWS’ unmatched experience, maturity, reliability, scalability and performance is the solid foundation for building organizations’ data-driven applications. AWS’ new product offerings will help organizations to be even more agile and innovate faster, cost-effectively, he said. Here are the new services and features announced at the AWS Summit. 1. Amazon Aurora Serverless v2 is generally available: instant scaling for demanding workloads: AWS says Amazon Aurora Serverless v2 scales capacity in fine-grained increments based on an application’s needs, all while including Amazon Aurora’s capabilities for high availability, performance, and resiliency, with low latency and fast querying. AWS claims Aurora Serverless v2 provides customers up to 90% cost savings compared to provisioning database capacity for peak loads. Some customers using this product include healthcare software provider, 3M and managed WordPress hosting platform, Pagely. 2. Amazon SageMaker Serverless Inference general availability – machine learning inference without worrying about servers: While Amazon SageMaker automatically provisions, scales, and turns off compute capacity based on volume of inference requests, the GA launch of Amazon SageMaker Serverless Inference allows customers to pay only for the duration of running the inference code and the amount of data processed. According to AWS, the elimination of idle time payments makes it ideal for applications with intermittent or unpredictable traffic, helping to scale compute capacity based on the volume of inference requests without a need for forecasting traffic demand up front or managing scaling policies. Customers include software and AI companies Bazaarvoice and Hugging Face, among others. 3. AWS IoT TwinMaker is now generally available: Amazon first debuted the AWS IoT TwinMaker last year. Today, the service is now generally available today in US East (Northern Virginia), US West (Oregon), Asia Pacific (Singapore and Sydney), Europe (Frankfurt), and Europe (Ireland) with availability in additional AWS regions coming soon. The AWS IoT TwinMaker makes it easier for businesses to create digital twins of real-world systems like buildings, factories, industrial equipment, and production lines. With IoT TwinMaker, customers like Siemens Digital Industries Software, Carrier, INVISTA, John Holland and others can use digital twins to build applications that mirror real-world systems that improve operational efficiency and reduce downtime. 4. AWS Amplify Studio is now generally available: AWS Amplify Studio is a new visual development environment for creating web application user interfaces (UIs) that extends AWS Amplify so developers can create fully customizable web applications on AWS with minimal coding. With Amplify Studio, developers can create a UI using a library of prebuilt components, collaborate with UX designers, and connect their UI to AWS services through a visual interface without writing any code. AWS Amplify Studio then converts the UI into JavaScript or TypeScript code, which saves developers time and energy while allowing them to customize their web application design and behavior using familiar programming languages. 5. Amazon Textract: Amazon Textract, a machine learning service that automatically extracts text, handwriting and data from any document or image, is announcing its newest offering to specify data and extract from documents using the new Queries feature. This feature allows users to specify the information needed in the form of natural language questions and receive the exact information as part of the API response. The feature uses a combination of visual, spatial, and language models to extract the information users seek with high accuracy. Some customers already using this product include cloud services and enterprise architecture company TekStream Solutions and supply chain management consulting firm Camelot Management Consultants. 6. Amazon QuickSight Embedded Analytics Partner Program: Amazon QuickSight — a cloud-native, serverless business intelligence (BI) service — is designed to help customers create and share interactive analytics with thousands of end-users, allowing interactive dashboards, visualizations, and ML-powered natural language queries to be embedded into apps and portals. The Amazon QuickSight Embedded Analytics Partner Program is designed to allow AWS customers to find trusted AWS partners that can help integrate Amazon QuickSight with hands-on assistance and long-term stability. Current partners include PwC, Rackspace and several others."
6,Amazon Aurora Serverless gets more granular and affordable,"April 21, 2022 12:30 PM",https://venturebeat.com/2022/04/21/amazon-aurora-serverless-gets-more-granular-and-affordable/,"Amazon Web Services (AWS) is updating its Amazon Aurora database to make the serverless configuration more practical and economical. In the second version of the serverless capability, Amazon Aurora Serverless v2, which will significantly reduce latency for scale up and scale down while making compute sizing far more granular — and affordable, according to AWS. Putting this in perspective, Amazon Aurora is a fully managed MySQL– and PostgreSQL — compatible database service. AWS mounts Aurora databases on its own optimized storage for higher scale and performance compared to the vanilla MySQL and PostgreSQL databases with conventional attached storage that it offers on its RDS service. AWS claims that its Aurora implementations are up to 5x faster than standard MySQL, and 3x faster than standard PostgreSQL implementations. Aurora storage is designed as a distributed storage system, with a minimum of six copies maintained across three availability zones (AZs) within a region. The key to Aurora’s performance is the intelligence baked into the storage layer. Aurora Serverless v2 is designed to reduce latency for scale-ups from minutes to under a second and scale-downs by up to 15x, while allowing more flexibility by supporting more granular compute sizing (expressed as Aurora Capacity Units, or ACUs). Like the original provisioned counterpart, Aurora Serverless v2 supports the same replica footprint across three AZs within a region, with 1-15 copies, depending on customer choice. The finer granularity and related enhancements are responsible for making autoscaling more efficient, making the process almost instant. It can ramp up to hundreds-of-thousands of transactions without disrupting the workload. Specifically, v2 of Aurora serverless can scale up or down in increments as small as 0.5 ACUs and 1 GByte of memory, to a maximum footprint of 128 ACUs including 256 GBytes of memory. There is one subtle distinction in the way that AWS implements serverless in Aurora. It will not scale down to zero because transaction databases are assumed to operate in always-on mode; in this case; that’s a much different use case from analytics, where it is not unusual to flat line, Amazon Aurora Serverless v2 keeps the lights on with a minimum 0.5 ACU and 1 GByte of memory; but customers can still choose to manually stop the instances when they are not in use. Conversely, customers can mix and match serverless and provisioned Aurora instances on the same cluster. For instance, customers who shard their databases, such as by line organization or region, can designate some shards with firm provisioned capacity and others for serverless. This is especially useful for organizations where workload patterns vary by workgroup or region. It even works within a single Aurora cluster where read replicas can be a mix of serverless and provisioned resources. So, why use serverless databases? Two triggers stand out. The first is the case for developer simplicity; just focus on developing the application and don’t worry about capacity planning or provisioning. The other is about traffic levels that are spikey by nature, or unpredictable. The cloud provides the opportunity for serverless because, unlike on-premises, you don’t have to buy just-in-case capacity. With all these benefits for serverless, provisioned is typically a better deal if traffic is relatively stable and predictable – cloud providers always give price breaks when you commit to fixed capacity up front. Serverless was originally associated with NoSQL operational databases that have few requirements for strong ACID transaction capability, and where traffic levels have often been less predictable than traditional transaction processing scenarios. AWS has always offered Amazon DynamoDB as serverless; significantly, many of AWS’s newer database offerings, including Amazon Timestream for time series; Amazon Keyspaces (for Apache Cassandra); and Amazon Quantum Ledger Database (QLDB) for immutable (blockchain-like) workloads, are also being offered serverless by default. Serverless is also offered either by default or as an option for NoSQL databases from Microsoft Azure, Google Cloud, and independents such as DataStax. But we are now seeing serverless expand outside its traditional NoSQL operational niche. Google Cloud broke ground when introducing BigQuery for analytics, originally only as a serverless service where you pay by query. Since then, they have added back in slotted engagements for organizations demanding more predictable monthly costs. And with Amazon Aurora, AWS has become one of the first to bring serverless to transaction processing. But it is no longer alone; a few weeks ago, Cockroach Labs joined the crowd by adding a serverless option to its distributed transaction database, initially within a single region. We expect going forward that serverless, as an option, will become a checkbox item, especially for cloud databases that support distributed deployment. While traffic to transaction databases was traditionally considered more predictable than, say, live mobile apps, a retailer or content provider that adds a new product or service that addresses a different market or demographic may find that some patterns of demand will also be less predictable. Being able to mix and match serverless with provisioned instances, such as what Amazon Aurora Serverless v2 will support, should fit the bill. Disclosure: AWS is a dbInsight client."
7,SageMaker Serverless Inference illustrates Amazon’s philosophy for ML workloads,"April 21, 2022 10:30 AM",https://venturebeat.com/2022/04/21/sagemaker-serverless-inference-illustrates-amazons-philosophy-for-ml-workloads/,"Amazon just unveiled Serverless Inference, a new option for SageMaker, its fully managed machine learning (ML) service. The goal for Amazon SageMaker Serverless Inference is to serve use cases with intermittent or infrequent traffic patterns, lowering total cost of ownership (TCO) and making the service easier to use. VentureBeat connected with Bratin Saha, AWS VP of Machine Learning, to discuss where Amazon SageMaker Serverless fits into the big picture of Amazon’s machine learning offering and how it affects ease of use and TCO, as well as Amazon’s philosophy and process in developing its machine learning portfolio. Inference is the productive phase of ML-powered applications. After a machine learning model has been created and fine-tuned using historical data, it is deployed for use in production. Inference refers to taking new data as input and producing results based on that data. For production ML applications, Amazon notes, inference accounts for up to 90% of total compute costs. According to Saha, Serverless Inference has been an oft-requested feature. In December 2021, SageMaker Serverless Inference was introduced in preview, and as of today, it is generally available. Serverless Inference enables SageMaker users to deploy machine learning models for inference without having to configure or manage the underlying infrastructure. The service can automatically provision and scale compute capacity based on the volume of inference requests. During idle time, it turns off compute capacity completely so that users are not charged. This is the latest addition to SageMaker’s options for serving inference. SageMaker Real-Time Inference is for workloads with low latency requirements in the order of milliseconds. SageMaker Asynchronous Inference is for inferences with large payload sizes or requiring long processing times. SageMaker Batch Transform to run predictions on batches of data, and SageMaker Serverless Inference is for workloads with intermittent or infrequent traffic patterns. SageMaker Serverless Inference comes on the heels of the SageMaker Inference Recommender service, introduced among a slew of AI and machine learning announcements at AWS re:Invent 2021. Inference Recommender helps users with the daunting task of choosing the best out of the 70 plus available compute instance options, and managing configuration to deploy machine learning models for optimal inference performance and cost. Overall, as Saha said, reducing TCO is a top priority for Amazon. In fact, Amazon has published an extensive analysis on the TCO of SageMaker. According to that analysis, Amazon SageMaker is the most cost-effective choice for end-to-end machine learning support and scalability, offering 54% lower TCO than other options over three years. Of note here, however, is what those “other options” are. In its analysis, Amazon compares SageMaker to other self-managed cloud-based machine learning options on AWS, such as Amazon Elastic Compute Cloud EC2 and Amazon Elastic Kubernetes Service EKS. According to Amazon’s analysis, SageMaker results in lower TCO when factoring in the cost of developing the equivalent of the services it offers from scratch. That may be the case, but arguably, users might find a comparison to services offered by competitors such as Azure Machine Learning and Google Vertex AI more useful. As Saha related, Amazon’s TCO analysis reflects its philosophy of focusing on its users, rather than the competition. Another key part of Amazon’s philosophy according to Saha is striving to build an end-to-end offering, and prioritizing user needs. Product development has a customer-driven focus: customers are consulted regularly, and it’s their input that drives new feature prioritization and development. SageMaker seems to be on an ever-growing trajectory, which also includes expanding the scope in terms of target audience. With the recent introduction of SageMaker Canvas for no-code AI model development Amazon wants to enable business users and analysts to create ML-powered applications as well. But what about Amazon’s double bottom line with SageMaker – better ease of use and lower TCO? As Tianhui Michael Li and Hugo Bowne-Anderson note in their analysis of SageMaker’s new features on VentureBeat, user-centric design will be key in winning the cloud race, and while Sagemaker has made significant strides in that direction, it still has a ways to go. In that light, Amazon’s strategy of converting more EC2 and EKS users to SageMaker and expanding the scope to include business users and analysts makes sense. According to a 2020 Kaggle survey, SageMaker usage among data scientists is at 16.5%, even though overall AWS usage is at 48.2% (mostly through direct access to EC2). At this point, it looks like only Google Cloud offers something comparable to Serverless Inference, via Vertex Pipelines.  At first glance, SageMaker seems more versatile as for supported frameworks, and more modular compared to Google Vertex AI – something which Saha also highlighted as an area of focus. Vertex Pipelines seems to correspond to SageMaker Model Building Pipelines, but is end-to-end serverless. As Li and Bowne-Anderson note, while Google’s cloud service holds a third-place ranking overall (behind Microsoft Azure and AWS), it holds a strong second place for data scientists according to the Kaggle Survey. The introduction of Serverless Inference plays into the ease of use theme, as not having to configure instances is a big win. Saha told VentureBeat that switching between different inference options is possible, and it’s done mostly via configuration. As Saha noted, Serverless Inference can be used to deploy any machine learning model, regardless of whether it has been trained on SageMaker or not. SageMaker’s built-in algorithms and machine learning framework-serving containers can be used to deploy models to a serverless inference endpoint, but users can also choose to bring their own containers. If traffic becomes predictable and stable, users can update from a serverless inference endpoint to a SageMaker real-time endpoint without the need to make changes to their container image. Using Serverless Inference, users also benefit from SageMaker’s features, including built-in metrics such as invocation count, faults, latency, host metrics and errors in Amazon CloudWatch. Since its preview launch, SageMaker Serverless Inference has added support for the SageMaker Python SDK and model registry. SageMaker Python SDK is an open-source library for building and deploying ML models on SageMaker. SageMaker model registry lets users catalog, version and deploy models to production. Ease of use may be hard to quantify, but what about TCO? Surely, Serverless Inference should reduce TCO for the use cases where it makes sense. However, Amazon does not have specific metrics to release at this point. What it does have, however, is early adopter testimonies. Jeff Boudier, director of product at Hugging Face, reports having tested Amazon SageMaker Serverless Inference and being able to significantly reduce costs for intermittent traffic workloads while abstracting the infrastructure. Lou Kratz, principal research engineer at Bazaarvoice, says that Amazon SageMaker Serverless Inference provides the best of both worlds, as it scales quickly and seamlessly during bursts in content and reduces costs for infrequently used models. SageMaker Serverless Inference has increased the maximum concurrent invocations per endpoint limit to 200 for the GA launch from 50 during preview, enabling use of Amazon SageMaker Serverless Inference for high-traffic workloads. The service is now available in all the AWS Regions where Amazon SageMaker is available, except for the AWS GovCloud (U.S.) and AWS China."
8,Element enhances digital twin data integration tools,"April 19, 2022 4:37 AM",https://venturebeat.com/2022/04/19/__trashed-57/,"Element Analytics, which creates IT and operational technology (OT) data management tools, has announced significant advances to support digital twin development. Specific improvements simplify data integration, enhance knowledge modeling and support data fusion from multiple sources.  The internet of things (IoT) often involves combining data from IT such as enterprise apps and OT such as industrial control systems. These tools have traditionally been created and managed by different teams and with different design goals. Element is designed to create tools that make it easier to connect OT data into digital twins managed on IT infrastructure spanning enterprise servers, cloud services, and various data management tools.  These tools help feed fresh, clean, organized and governed data to digital twins to provide more accurate, timely and valuable models of assets. This requires bringing together data from various sources, including operational data, which is often inconsistently labeled and lacks metadata context. Element’s Unify data platform automates these DataOps processes and organizes this data into a knowledge graph.  Element customers span the chemical, power, utility, food and agriculture sectors. For example, Nova Scotia Power uses Unify to import and contextualize data from five multi-unit coal generation thermal plants. They were able to construct a comprehensive asset framework representing their data hierarchy. The project then evolved to help with analytics and reporting via a web-based portal for a complete set of generating assets as the utility transitions to include greener, wind-based generation.  Evonik, a leading specialty chemical company, uses Unify’s tools to create a 360-degree view of equipment for plant operations teams. The company has reduced the time and effort to contextualize the complex data streams produced by its plant equipment. It estimates saving $110 thousand from a single analytics model at just one plant and expects considerably more savings with a broader rollout.  Here is what the new enhancements mean for digital twins: Connector portal: The Unify Connector Portal provides pre-built connectors for commonly used data sources and destinations used to feed data into digital twins. This eliminates the need to code individual connections and speeds and simplifies the process. All the user needs to do is select the connector from the portal, register it by entering details in a dialog box, and then deploy it.  Unify Graph: Graphs are a flexible approach to describing various entities and their relationships used to represent the different elements within a digital twin. Unify Graph allows a user to represent different data sources and how they relate to process flows, asset hierarchies, and organization structures. The user can quickly build on their initial graph by adding more sources and associated relationships incrementally. They can explore the graph visually and query it using a graph query language within Unify or export the graph-to-graph database products such as AWS Neptune or Neo4j. Unify can export its graph models for use by digital twin tools such as AWS IoT TwinMaker. Advanced joins: Digital twins are crafted from joining data from multiple sources to represent different aspects of a thing or process. Joining data involves identifying overlapping fields from different data streams. But in real-world scenarios, there are often subtle differences in how this data is encoded across sources owing to different naming conventions or lack of compliance. Data engineers must then manually wrangle this data together to join data sets or data streams to account for such differences. New join techniques support “fuzzy” and “contains” joins that do a better job at accurately matching up uncertain data.  A few enterprises have seen impressive benefits from digital twins. For example, the Abu Dhabi National Oil Company used digital twins to generate $1 billion in additional revenue. But other industrial clients are less mature in their digital twin efforts.  “It is relatively early for most of our clients within the industrial sector when it comes to using advanced graph database processing and graph-based AI techniques,” Element CEO Andy Bane told VentureBeat Bane says the industry needs to do more work on improving basic graph data processing capability for tasks like characterizing the connections or relationships between disparate data sources. Once this basic graph model is in place, it will be easier for enterprises to build more advanced digital twins.  There are industry efforts to promote formats such as Digital Twins Definition Language (DTDL), though Bane is unsure whether these standards will achieve widespread adoption.  “Ultimately, what matters to the business is the ability to establish robust models and reliable data pathways, so that the digital twin has fresh, clean, organized, and governed data that it can operate upon to support decision-making,” he said. “Given the reality of the diverse deployed plant and associated operational technology at major industrials, digital twin success calls for the ability to bring that data to the digital twin efficiently and reliably, regardless of the source format.”"
9,VAST Data teams up with Vertica to enable real-time queries at scale,"April 19, 2022 12:01 AM",https://venturebeat.com/2022/04/19/vast-data-teams-up-with-vertica-to-enable-real-time-queries-at-scale/,"The data lake is by now a familiar term and concept across the enterprise, as is the data warehouse. But what about “data lakehouse?” As its name would suggest, the emerging architecture fuses the functions of the data lake repository for raw data with the reporting and analysis business intelligence (BI) of the data warehouse. Also referred to as the less catchy “unified analytics warehouse,” this architecture can manage an organization’s full complement of structured, semi-structured and unstructured data. It can support many different data workloads and has the ability to be deployed on top of low-cost cloud storage systems. “It comes down to insight,” said Jeff Denworth, cofounder and chief marketing officer of flash memory data storage company VAST Data. “It provides one comprehensive view across an entire data estate.” To provide this vantage point for companies and enable real-time queries at scale, VAST Data has partnered with database company Vertica. The partnership announced today unites VAST Data’s all-flash Universal Storage data platform with Vertica’s Eon Mode Architecture to create an all-flash data lakehouse. This helps enterprises consolidate their structured and unstructured data silos to democratize data for real-time data exploration, analytics and insights, Denworth said. “Customers can start to run a lot more queries,” he said, “they can get much faster query responses.” The market for managing big data only continues to grow as organizations amass data on larger and larger scales. Global Industry Analysts, Inc., has forecasted it to reach $234.6 billion by 2026. Growing right along with it is the list of companies supporting data lakehouse architectures. These include big data giants Snowflake and Databricks, as well as Oracle Cloud Infrastructure (OCI) and Google, which preview launched its BigLake engine at its Cloud Data Summit earlier this month. Onehouse emerged from stealth in February with its open-source data lakehouse; Dremio recently raised $160 million in series E and in March released a free edition of its SQL lakehouse. Databricks, which was founded in 2013 and reports an estimated $38 billion post-money valuation, has said that 5,000 global organizations leverage its Databricks Lakehouse Platform. Snowflake is helping companies with page-level intelligence under GDPR/CCPA compliance and with collecting, storing and preparing data to support multiple use cases. Gerrit Kazmaier, vice president and general manager of Databases, Data Analytics and Business Intelligence at Google Cloud, said of its decision to enter the market: “Managing data across disparate lakes and warehouses creates silos and increases risk and cost, especially when data needs to be moved.” With their partnership, VAST and Vertica aim to provide a unique offering in a growing field of competitors. As Denworth pointed out, in the case of data, one immense problem for enterprise is compartmentalized storage. Historically, companies have built multiple data warehouses and multiple lakes of that data, thus resulting in siloes. Then, when they’ve asked a question of data, he said, they haven’t necessarily received relevant responses, or they have dealt with response times that are very challenging based upon the orientation of their data. “If you want to ask a question and get the greatest possible answer, you really need to look across all of the data as it comes in an organization,” Denworth said. “Historically, that’s been really challenging because no one system is designed to essentially see everything.” Thus, the data lakehouse is designed to provide enhanced insight from broader looks at data. This is an incredibly valuable tool, he said, for big data teams and data science organizations that are trying to be broader and more flexible with their data analysis. “Now you don’t have to copy data from department to department to department,” Denworth said. “You just make these stateless servers and they all have access to the same data underneath.” For instance, the new VAST-Vertica-enabled lakehouse is being leveraged by Singapore online travel agency Agoda to support and enhance its recommendation engine. A mobile casino game company is also using the architecture for its recommendation capabilities. Typically, Denworth said, organizations think they have to go to the cloud to get the best lake-warehouse solution. Or, if they look on-premises, their options are systems that are large and slow, or small and “very expensive and very fast.” “Flash is something that marries both: big, cheap and fast,” he said. Customers transitioning to the VAST-Vertica lakehouse model can save 80% to 90% while supporting capacity scale by factors of 100, Denworth said. He emphasized the fact that the average data warehouse houses terabytes in factors of 10. But companies that move beyond the data warehouse model to the lakehouse model speak extensively in petabyte terms. “Our customers are directing huge, huge datasets into the system,” Denworth said."
10,The challenge of integrating new data and collection tech with historic data,"April 15, 2022 11:07 AM",https://venturebeat.com/2022/04/15/the-challenge-of-integrating-new-data-and-collection-tech-with-historic-data/,"Any time new technologies or systems become too valuable or ubiquitous not to integrate for businesses across industry, there are holdouts that cling to the old ways or prioritize the familiar over the innovative. Those organizations tend not to last long. But even among adopters, there are those companies that try and fail to merge the old with the new, while others make it happen. We’re seeing this on full display in the areas of sports, where organizations are challenged to integrate legacy data with new collection technologies and data sets. What sets the success stories apart? When faced with waves of new data due to advancements in automation and data collection methods, a sports organization should first acknowledge that it’s a good problem to have. With technology like lidar, for example (a laser-based movement-tracking system), that is focused on improving the accuracy, depth of information and seamlessness of data collection, performance evaluators now have access to an enormous, untapped trove of data that can be used to better inform their decisions. The question then becomes: how does a club manage that influx of new data? First, preach patience. Consider that organizations and their data teams have been using the same methods and approaches, making the same assumptions and associations, for years. Old habits die hard. And because advanced analytics can be applied to everything from game strategy to the optimal types of soda served at the stadium concession stands, an organization adopting these technologies for the first time will need across-the-board buy-in. That takes time. The biggest challenge, however, is integrating an organization’s historic data with modern information. Collection technologies and methods aren’t all that have changed in this area. Today’s data looks very different than that of the past, and in some cases, the types of measurements don’t align with previous data sets. How do an organization’s data teams solve this problem? Start here: The key for sports organizations integrating old and new technologies, methodologies and information is to take a deep, thorough dive into the data. Raw historical data don’t help most clubs. Data needs to be easily understood by new user profiles down to make it viable, which takes valuable time and which may leech all its usefulness in the process. A schism may exist between data sets tracking similar or identical movements using different technologies or approaches. When measuring the force of a kick on the pitch, for instance, data collected from wearables attached to a player’s boot may not easily integrate with data collected that measured that same kick using laser-based lidar.  And because wearable technologies are limiting in where and how often those measurements can be tracked, there may be gaps in the feedback from the tech due to missing data points. Data smoothing can’t stitch this information together. Upgrading to new technologies is, of course, often worth it. Take lidar, which is more accurate while being more portable and unobtrusive from the player’s standpoint than past tech. The challenge of data integration is the only noteworthy downside to adopting lidar for a club’s player evaluation department. And with the right plan, even that challenge can be solved. Raf Keustermans is the CEO of Sportlight Technology."
11,Bad data: A $3T-per-year problem with a solution,"April 14, 2022 11:07 AM",https://venturebeat.com/2022/04/14/bad-data-a-3t-per-year-problem-with-a-solution/,"A few years ago, IBM reported that businesses lost $3 trillion dollars per year due to bad data. Today, Gartner estimates $12.9 million to be the yearly cost of poor-quality data. Funds get wasted in digitizing sources as well as organizing and hunting for information — an issue that, if anything, has increased now that the world has shifted to more digitized and remote environments.  Apart from the impact on revenue, bad data (or the lack of it) leads to poor decision-making and business assessments in the long run. Truth be told, data is not data until it is actionable, and to get there it must be accessible. In this piece, we’ll discuss how deep learning can make data more structured, accessible and accurate, avoiding massive losses on revenue and productivity in the process.  Every day, companies work with data usually filed as scanned documents, PDFs or even images. It’s estimated that there are 2.5 trillion PDF documents in the world, however, organizations continue to struggle with automating the extraction of correct and relevant quality data from paper and digital-based documentation — which usually results in unavailable data or in productivity problems given that slow extraction processes are not a match for our current digital-driven world.  Although some may think that manual data entry is a good method for turning sensitive documents into actionable data, it’s not without its faults, as they expose themselves to increased chances of human error and the consequent costs of a time-consuming task that could (and should) be automated. So, the question remains, how can we make data accessible and accurate? And beyond that, how can we capture the correct data easily, while reducing the manual-intensive work?   Machine learning has been on the path to revolutionize everything we do during the past few decades. Its goal from the get-go has been to utilize data and algorithms to imitate the way that we humans learn – and from there, gradually learn our tasks to improve their accuracy. It’s no surprise that advanced technologies have been greatly adopted amid the digital revolution. In fact, we’ve landed on the point of no return, considering that by 2025, the amount of data generated each day is expected to reach 463 exabytes globally. This is simply a reflection of the urgency around creating processes that can withstand the future.   Technology today plays an integral role in the upkeep and quality of data. Data extraction APIs, for example, have the ability to make data more structured, accessible, and accurate, altogether increasing digital competitiveness. A key step in making data accessible is enabling data portability, a concept that protects users from locking in their data, in “silos” or “walled gardens” that may be incompatible with one another, thus subjecting them to complications in the creation of data backups.   Luckily, there are steps to consider for utilizing the power of machine learning for data portability and availability at an organizational level.   The truth is, data can’t help you if it’s not accessible: you can’t automate processes if data isn’t recognizable and usable by a machine. It is a complex process that, when done well, brings a lot of benefits including accelerating the gathering of insights for faster decision making, providing higher productivity by facilitating faster data retrieval, improving accuracy through AI/ML and end-user experience and reducing overall costs of manual data extraction.   Organizations may be rich in data, but the reality is that data serves no purpose if users cannot interact with it at the right time. As we all know, most work-specific processes start with a document. However, how we treat these documents has changed, removing the human focus from inputting data and shifting it to controlling data to ensure processes run smoothly.   True decision-making power lies in being able to pull company information and data quickly while having peace of mind that the data will be accurate. This is why controlling data holds an enormous value. It ensures the quality of the information being used to build your business, make decisions and acquire customers.   Technology has given us the possibility to let automation do the more mundane, yet important admin tasks so that we can focus on bringing real value — let’s embrace it. After all, data must be actionable. As you continue in your digital transformation journey, remember that the more (accurate) data you send a machine learning model, the better the results you will receive. Jonathan Grandperrin is the cofounder CEO of Mindee."
12,Conversational AI explodes to fulfill CX gap,"April 12, 2022 10:10 AM",https://venturebeat.com/2022/04/12/conversational-ai-explodes-to-fulfill-cx-gap/,"COVID-19 has led to a dramatic acceleration in the adoption and implementation of digital transformation initiatives. Nowhere was this more obvious than in customer experience (CX). Organizations have been quick to adopt new technologies such as chatbots powered by artificial intelligence (AI) to fulfill customer expectations of timely response to queries and problem resolution.  >> Read more in the Data Pipeline << Chatbots are an example of how AI can be used to augment human capabilities, providing a convenient way for customers to interact with organizations 24/7. In the customer service context, they can provide an efficient and cost-effective way to handle large volumes of customer inquiries. This frees up human agents to focus on more complicated queries. With the increase in customer demand for digital channels during COVID-19, organizations that have invested in chatbots have been able to scale customer support and address prospect queries. In the customer service world, “automation” has been a dirty word. The customer wants to talk to a human being, not a machine. Employees bristle at the thought of being replaced by an automated system. However, as voice search, smart speakers and voice assistants gain adoption and acceptance, automation is becoming indispensable for providing an excellent CX.  Conversational AI allows customers and employees to get the answers they need quickly and easily, without having to wait on hold or jump through hoops. On the back end, total-experience automation brings all of your customer, product and employee data together in one place. This makes it easy to track customer journeys, spot areas for improvement and deliver consistently accurate responses to queries. In today’s customer-centric world, total-experience automation is essential for providing an outstanding CX and employee experience (EX). Conversational AI allows human-like conversations between users with websites, applications and devices via texts, voice and commands. The five most important elements that come together to make conversational AI a reality are the following: When all components are in place, the conversational AI experience can tap into many of the aspects that make the human language such a versatile and rich communication medium. Total-experience automation is an approach to CX and EX that involves automating conversations across platforms on the front end and integrating with enterprise systems on the back end. The goal of total-experience automation is to provide a more seamless, efficient and personalized CX and EX by using conversational AI to automate interactions. The challenge is to improve the quality of the experience while managing costs in an era of exploding data, channels, customer expectations and employee turnover. To do this, businesses need to have a robust back-end infrastructure in place that can connect customer data from various sources and enable real-time communication between front- and back-end systems. By doing so, they can provide a more cohesive CX and empower employees to focus on higher-value tasks. In addition, total-experience automation can help businesses reduce costs by automating low-value, repetitive tasks such as forecasting demand and mitigating supply chain disruptions. Yellow.ai is leading the way in conversational AI by providing a highly efficient interface with stakeholders. They currently have more than 1,200 customers across financial services, retail, energy, education and gaming including Unilever, P&G, Schlumberger, Roche and Amazon.  The core of the front-end is multilingual. It supports more than 100 languages across more than 35 text and voice channels including WhatsApp, Google Business Messages, Apple Chat, Instagram, Messenger, Viber, WeChat, Alexa, Telephony and others The platform is built on a proprietary NLP engine. The Natural Language Understanding (NLU) and NLP engine compound the self-learning of the Dynamic AI Agents through multifactorial intent recognition, effective engagement and on-point resolution – all in real-time and with 98% accuracy.  The predictive/AI layer predicts the future conversation or allows third-party tools to predict conversation and manage workflow. This enables end-users to place orders which in turn allows partners to use data feeds to build models for future predictions with little data or feature engineering. Diageo has a bartender assistant that helps bartenders make cocktails and concurrently helps with demand management using predictive analysis. Asian Paints customer interactions create real-time predictions of inventory needs. American Bureau of Shipping customers place orders and get status updates throughout the shipping, transportation and delivery process which is used to predict shipping demand. AI and automation have the capability to ingest and analyze huge amounts of information and data in milliseconds. Yellow.ai provides an interface between the customer and backend systems. It skips over the front office while also supplying an NLP interface. Dynamic AI agents uniquely learn from all human-answered queries to rapidly decrease future AI-to-human hand-offs, achieving 60% automation in the first 30 days of go-live. Humans have emotional and empathetic abilities. While Yellow.ai doesn’t try to synthesize that, it does have the ability to bring in humans at the right point of the interaction. This creates training points to improve the models.  For example, when it comes to complex conversations that need empathy, the end-user is looking for human intervention. As for a customer looking for details on COVID-19 protocols, they want to connect with a human. This is also true for conversations that might involve customers at risk of churning or high-value transactions. The key to overcoming these challenges and creating an effective conversational AI strategy is to enable seamless handoff to human beings, thereby, leveraging the collaborative intelligence of humans and AI. The more seamless and successful these transitions become, the faster we’ll see the adoption and acceptance of conversational AI chatbots in all industries."
13,"What is dirty data? Sources, impact, key strategies","April 11, 2022 6:40 AM",https://venturebeat.com/2022/04/11/what-is-dirty-data-sources-impact-key-strategies/,"Enterprise data is critical to business success. Companies around the world understand this and leverage platforms such as Snowflake to make the most of information streaming in from various sources. However, more often than not, this data can become “dirty.” In essence, it could, at any stage of the pipeline, lose key attributes such as accuracy, accessibility and completeness (among others), becoming unsuitable for downstream use initially targeted by the organization. “Some data can be objectively wrong. Data fields can be left blank, misspelled or inaccurate names, addresses, phone numbers can be provided and duplicate information…are some examples. However, whether that data can be classed as dirty very much depends on context. For example, a missing or incorrect email address is not required to complete a retail store sale, but a marketing team who wishes to contact customers via email to send promotional information will classify that same data as dirty,” Jason Medd, research director at Gartner, told VentureBeat.   In addition, the untimely and inconsistent flow of information can also add to the problem of dirty data within an organization. The latter particularly occurs in the case of merging information from two or more systems using different standards. For instance, if one system classifies names as a single field while the other divides them into two, only one will be considered valid, with the other requiring cleansing. Overall, the entire issue boils down to five key sources: As Medd explained, dirty data can occur due to human errors upon entry. This could be an outcome of shoddy work from the person entering the data, the lack of training or poorly defined roles and responsibilities. Many organizations do not even consider establishing a data-focused collaborative culture  Process oversight can also lead to cases of dirty data. For instance, poorly defined data lifecycles could lead to the use of outdated information across systems (people change numbers, addresses over time). There could also be issues due to the lack of data quality firewalls for critical data capture points or the lack of clear cross-functional data processes. Technology glitches such as programming errors or poorly maintained internal/external interfaces can affect data quality and consistency. Many organizations can even miss out on deploying data quality tools or end up keeping multiple varying copies of the same data due to system fragmentation. Among other things, activities at the broader organization level, such as acquisitions and mergers, can also disrupt data practices. This issue is particularly common in large enterprises. Not to mention, due to the complexity of such organizations, the head of many functional areas could resort to keeping and managing data in silos.  Gaps in governance, which ensures authority and control over data assets, could be another reason for quality issues. Organizations failing to set data entry standards, appointing data owners/stewards or placing broken policies for scale, pace and distribution of data could end up with botched first and third-party data. “Data governance is the specification of decision rights and an accountability framework to ensure the appropriate behavior in the valuation, creation, consumption and control of data. It also defines a policy management framework to ensure data quality throughout the business value chains. Managing dirty data is not simply a technology problem. It requires the application and coordination of people, processes and technology. Data governance is a key pillar to not just identifying dirty data, but also for ensuring issues are remediated and monitored on an ongoing basis,” Medd added. Whatever the source, data quality issues can have a significant impact on downstream analytics, resulting in poor business decisions, inefficiencies, missed opportunities and reputational damage. There can also be smaller problems such as sending the same communication message multiple times to a customer whose name was recorded differently in the same system.  All this eventually translates into additional costs, attrition, bad customer experiences. In fact, Medd pointed out that poor data quality can cost organizations an average of $12.9 million every year. Stewart Bond, the director of data integration and intelligence research at IDC, also shared the same opinion, noting that his organization’s recent data trust survey found that low levels of data quality and trust impact operational costs the most. In order to keep the data pipeline clean, organizations should set up a scalable and comprehensive data quality program covering the tactical data quality problems as well as strategic aspects of the alignment of resources and business objectives. This, as Medd explained, can be done by building a strong foundation bolstered by modern technology, metrics, processes, policies, roles and responsibilities.  “Organizations have typically solved data quality problems as point solutions in individual business units, where the problems are manifested most. This could be a good starting point for a data quality initiative. However, the solutions frequently focus on specific use cases and often overlook the broader business context, which may involve other business units. It’s critical for organizations to have scalable data quality programs so that they can build on their successes in experience and skills,” Medd said. In a nutshell, a data quality program has to have six main layers: As part of this, the organization has to define the broader goal of the program, detailing what data they plan to keep under the scanner, which business processes can lead to the bad data (and how) and which departments’ can ultimately be impacted by that data. Based on this information, the organization could then define data rules and appoint data owners and stewards for accountability. A good example could be the case of customer records. An organization with the goal to ensure unique and accurate customer records for use by marketing teams can have rules like all addresses and names gathered from fresh orders should be unique when put together or the addresses should be verified against an authorized database.  Once the rules are defined, the organization has to use them to check new (at source) and existing data records for key quality attributes, starting from accuracy and completeness to consistency and timeliness. The process usually involves leveraging qualitative/quantitative tools, as most enterprises deal with a large variety and volume of information from different systems. “There are many data quality solutions available in the market, that range from domain-specific (customers, addresses, products, locations, etc.) to software that finds bad data based on the rules that define what good data is. There is also an emerging set of software vendors that are using data science and machine learning techniques to find anomalies in data as possible data quality issues. The first line of defense though is having data standards in place for data entry,” IDC’s Bond told Venturebeat. Following the assessment, the results have to be analyzed. At this stage, the team responsible for the data has to understand the quality gaps (if any) and determine the root cause of the problems (faulty entry, duplication or anything else). This shows how far off the current data is from the original goal targeted by the organization and what needs to be done moving ahead. With the root cause in sight, the organization has to develop and implement plans for solving the problem at hand. This should include steps to correct the issue as well as policy, technology or process-related changes to make sure that the problem doesn’t occur again. Note here that the steps should be executed by taking resources and costs into account, and some changes might take longer to be implemented than others. Finally, the organization has to ensure that the changes remain in effect and the data quality is in line with the data rules. The information around the current standards and status of the data should be promoted across the organization, cultivating a collaborative culture to ensure data quality on an ongoing basis."
14,"Google Cloud federates warehouse and lake, BI and AI","April 6, 2022 6:00 AM",https://venturebeat.com/2022/04/06/google-cloud-federates-warehouse-and-lake-bi-and-ai/,"Google Cloud is making a series of announcements today, covering a range of its data, analytics and AI services. A combination of preview and general availability (GA) releases are being launched today that, together, will shore up Google’s data and AI story, as it competes with Amazon Web Services (AWS) and Microsoft Azure. In a blog post, Gerrit Kazmaier, Google Cloud’s GM for databases, data analytics, and Looker said “With the dramatic growth in the amount and types of data, workloads, and users, we are at a tipping point where traditional data architectures — even when deployed in the cloud — are unable to unlock its full potential. As a result, the data-to-value gap is growing.” Perhaps in response, the overarching theme to Google’s announcements today is bringing things together. Google Cloud’s data warehouse and data lake will be more integrated; Google’s organically developed business intelligence (BI) components will work in a more coordinated way with the Looker BI technology that Google acquired in 2020; and Google’s analytics and AI components will work together more seamlessly as well. Perhaps the most important of today’s announcements is the launch in preview of a new data lake offering, called BigLake. As you might imagine from the name, this service will make data lakes stored in Google Cloud Storage (GCS) far better integrated with BigQuery, Google Cloud’s data warehouse service. Not only will Google Cloud customers be able to query data in the lake and warehouse together, from services like Spark, Presto and even TensorFlow, but the security and governance of data in the lake and the warehouse can be unified as well. This coordination of lake and warehouse will resonate with fans of the so-called lakehouse model, while still respecting that data lake and data warehouse technologies each have relative strengths. In other words, customers will have a choice of which data to store where, and can still have a unified query and governance experience. GA of this service will likely come by the end of the calendar year. Google is also announcing something called Spanner change streams, a change data capture service that will replicate data in real time from Google Cloud Spanner into BigQuery, Pub/Sub or Google Cloud Storage. This offering seems quite comparable to Microsoft’s Azure Cosmos DB change feed. This service isn’t available yet, but Google says it’s “coming soon.” Six years ago, Google brought out its self-service BI product called Google Data Studio, making it easy for business users to create visualizations on data stored in a variety of repositories and platforms. Later, extensions were made to make Google Sheets more data-savvy, too. But then Google Cloud acquired indie BI player Looker as well, leaving customers and industry journalists (including this one) to wonder what the future held for Data Studio. Google is clarifying that story today, explaining that Google Data Studio can now connect to data contained in Looker models, and that Google Connected Sheets can do likewise. Looker, you see, includes the Explore data query and visualization front-end, but it also has a back-end of sorts, allowing customers to create comprehensive models that blend data from different sources, and which define the elements of that blended data that constitute the model’s measures (metrics) and dimensions (categories, like product, time, and location, used to aggregate or drill down on the metrics). Looker models are created in a special language called LookML (the “ML” stands for markup language, not machine learning) and those models will now be readable by Google Data Studio and Google Sheets, allowing them to serve developers, enterprise BI analysts, self-service BI business users and spreadsheet users as well. Google has, for quite some time, seen itself as the leading contender to create the first-class cloud for artificial intelligence (AI). And while the company’s AI prowess is quite apparent, Google Cloud’s AI was until recently more a collection of individual services. The assortment included a cloud TensorFlow service, an array of Web API-based cognitive services, and an in-database AI service called BigQuery ML (where, this time, the ML does stand for “machine learning”). Meanwhile, Microsoft’s Azure Machine Learning and AWS’ SageMaker were offering more integrated machine learning platforms, even if sometimes by virtue of a common brand. Google’s answer to this was its Vertex AI service, released to general availability in May of last year. And here again, Google Cloud is focusing on cohesion and integration. An important part of the service, Vertex AI Workbench, being released to GA today, integrates natively with BigQuery, Serverless Spark, and Dataproc. Today, Google is adding a new Model Registry to Vertex AI. Think of a model registry in the machine learning world as comparable to a data catalog in the database and analytics world, in that it’s a searchable, central repository and governance tool for all of an organization’s machine learning models. Google also points out, maintaining that overarching theme of unification, that the model registry will catalog models living both in Vertex AI and in BigQuery ML. What’s interesting about all of Google’s announcements today, is how reminiscent they are of patterns that have shown up in the analytics and BI worlds already. For example, creating a side-by-side data warehouse/data lake environment is very much like what Microsoft’s Azure Synapse Analytics had done already: bring together the former Azure SQL Data Warehouse with Azure Data Lake Storage, Spark and a data lake query engine. On the BI side, bringing together native and acquired technologies is very reminiscent of what Microsoft, IBM, SAP, and Oracle did back in the 2000s when they made their own BI acquisitions, of ProClarity, Cognos, BusinessObjects and Hyperion, respectively. Even the notion of Google using Looker’s semantic layer technology to glue it together with Data Studio and Connected Sheets is not unprecedented. To this day, BusinessObjects “Universes,” also a semantic data model technology, are a centerpiece of SAP’s BI story, both on-premises and in the company’s Analytics Cloud service. In many ways, the major cloud providers of today mirror the enterprise “mega vendors” of fifteen to twenty years ago. And, fittingly, Google Cloud’s data and analytics announcements today show that the enterprise stack model is very much alive, even in the era of the cloud."
15,Grafana Labs reaches a fork in the open source road,"April 6, 2022 5:00 AM",https://venturebeat.com/2022/04/06/grafana-labs-reaches-a-fork-in-the-open-source-road/,"The company behind Grafana, one of the fastest growing open source projects, has just secured a new round of funding that values the company at upwards of $5 billion. It can thank Splunk and Elastic for clearing the way. And, like a number of its counterparts, Grafana Labs has gone down the quasi or restricted open source relicensing path. That path, by the way, is growing forkier by the day. First, the headline: Grafana Labs, the company, has just secured a $240 million series D round of funding, and with it, inflating its valuation. It comes for a company that estimates that its eponymous open source project grew by roughly 25 times throughout the past six years, with the installed base now exceeding 900,000 installations and 10 million global users. We’d expect the company to do the usual thing for a D round, which is expanding its go-to-market to generate revenue, but the company also maintains that it will continue expanding the product’s capabilities. It follows an announcement the previous week for a major change to one of Grafana Labs’ longtime supporting projects, the Cortex time series database. And this is where the fork begins. Grafana Labs is replacing Cortex, which had been Apache 2 licensed, with Mimir, which is broader in scope, and licensed – just like Grafana and companion projects Loki and Tempo – under the more restrictive AGPLv3. It’s a form of “copyleft” license that in this case is intended to deter third-parties from taking the law into their own hands and forking the code. It requires them to contribute any changes they make back into the trunk. There’s a long and involved saga over open source licensing Sturm und Drang that we have chronicled here, here and here. For Grafana Labs, there are several ironies. The first is that MongoDB famously ditched the AGPL license because it wasn’t restrictive enough; instead, the company went to SSPL, which is more explicit about prohibiting anyone from offering a MongoDB cloud service. Others, such as Confluent, Cockroach Labs and Redis, have taken similar restrictive approaches for all or parts of their portfolios. Secondly, while the target of such licensing more often than not is AWS, in this instance, it’s a case of competition as both companies have had a unique partnership with Amazon Managed Grafana service. A related AWS service that integrates to the Grafana offering, Amazon Managed Prometheus, continues to use Cortex (with a couple of AWS employees maintaining it). Some are questioning whether this collaboration is continuing. There’s also a saga over going to version 2. Fortunately, Mimir is an API-compatible superset of Cortex, so existing customers won’t be thrown under the bus; the migration path should be pretty straightforward. Grafana Labs characterizes Mimir as a natural progression from Cortex. In effect, it chose to borrow more capabilities, such as support for a sharded query engine; an API for identifying high cardinality metrics; and broader metrics ingestion capabilities that were previously only available in its cloud SaaS service. The most advanced capabilities, such as compatibility with Graphite and Datadog, finer-grained access controls, cross-cluster query federation and so on, are still only available from Grafana, through its cloud service or the on-premises counterpart, Grafana Enterprise Metrics. There’s a lot of history repeating itself with Grafana Labs’ trajectory. The great-grandparent of all this was Splunk, which provided a developer-friendly, but proprietary platform for monitoring, searching and analyzing the types of machine data generated by IT systems. Splunk was designed for an era of systems that were deployed inside the data center. Then the cloud, and the massive scale of it, changed things. And so emerged Elasticsearch, an open source alternative that was designed from the get-go for highly distributed operation. Under its original licensing, Splunk users paid for larger deployments, typically by the number and size of indexes. Elasticsearch, as open source, did not penalize for size. Elastic, the company behind Elasticsearch, became a victim of its success as virtually every cloud provider mounted its own Elasticsearch services. That prompted Elastic to revamp it’s licensing until finally, a couple of years back, it bit the bullet and stopped being an open source company altogether. Meanwhile, AWS Apache open-sourced the Elasticsearch version under the original license and forked to the new OpenSearch project. Don’t shed too many tears for Elastic, as, having already successfully IPO’ed, it is on track for an $800 million year, representing a roughly 25% to 30% increase year over year. Grafana Labs has often been viewed as a company whose technologies compete or sometimes work alongside Elasticsearch. The main difference is Grafana originated as a fork of Kibana (the visualization piece of the Elastic ELK stack), and it was better suited for monitoring the types of metrics that are at the heart of observability. In this case, competition was the sincerest form of flattery, as Kibana subsequently upgraded its metrics support.  With the shift from Cortex to Mimir, the type of forking that came to Elastic is now coming to Grafana. We view forking as a rite of passage for open source projects, as it shows there’s sufficient interest to drive competing directions; projects like MongoDB and PostgreSQL have not exactly suffered from forking. Given the traction of Grafana, if history is any guide, expect more forks in the road ahead."
16,Oracle cranks up MySQL HeatWave’s thermostat for in-database machine learning,"April 1, 2022 5:10 AM",https://venturebeat.com/2022/04/01/oracle-cranks-up-mysql-heatwaves-thermostat-for-in-database-machine-learning/,"Since acquiring Sun Microsystems well over a decade ago, Oracle has owned MySQL. Under Oracle’s watch, MySQL has remained distinct. But unless you were MariaDB, until a couple years or so ago, few gave Oracle’s stewardship much thought. And as each of the major cloud providers rolled out its own managed MySQL database services, Oracle provided relatively few reasons to draw customers to MySQL. Well, that’s no more. Fifteen months ago Oracle introduced MySQL HeatWave featuring its own optimized implementation of MySQL running in Oracle Cloud Infrastructure (OCI, aka Oracle’s public cloud platform). Those optimizations should be transparent to the application. And now Oracle is making the 3.0 release of HeatWave, scaling up node size that will reduce costs for a number of workloads, and introducing in-database machine learning, which could benefit from higher density data nodes. HeatWave isn’t plain vanilla open source MySQL, as it differentiated with Oracle-developed extensions (outlined below). That’s not particularly unusual in open source, as Amazon Aurora and Azure PostgreSQL Hyperscale, not to mention the countless other PostgreSQL variants on the market, show that open source databases provide clean slates for differentiation. In making its move to become a serious competitor in the MySQL space, Oracle took the database in a unique direction with HeatWave: It optimized for analytics in addition to transaction processing by taking advantage of MySQL’s support for pluggable storage engines. In this case, it plugged in an in-memory columnar storage engine that operates side by side with the row store, incorporating optimizations tailored for processing analytic queries.  Plugging in a columnar storage engine working side by side with a row-oriented engine isn’t unusual; MariaDB has done it and, in fact, Oracle took a similar path but with different technology for its flagship database several years ago. But to this day, Oracle is the only one to pull off an analytic-optimized engine for MySQL. In the latest release, Oracle has introduced enhancements to reduce compute costs and bring machine learning in-database. Let’s start with operating costs. HeatWave version 3.0 doubles data density in each compute node without changing the pricing. So you can now consume (pay for) just half the number of nodes to compute the same workload. And, by the way, Oracle set the stage for all this in the previous HeatWave 2.0 release where it doubled the maximum upper limit for HeatWave clusters to 64 nodes. Combined, compute cost efficiencies and scale should come in handy now that machine learning models can be run in-database. Hold that thought. Beyond data density, HeatWave 3.0 makes it more economical to scale, in that you can add any number of nodes (up to a maximum of 64) in any increment. This is consistent with what Oracle introduced for its Autonomous Database cloud service, getting rid of the so-called standard “T-shirt sizes.” So elasticity with HeatWave doesn’t mean that you have to double the number of active nodes each time your workload bursts compute. HeatWave also improves availability while resizing, with at most a few microseconds while querying is paused. HeatWave 3.0 is adding a few tricks to further speed up processing. Like any columnar storage engine, HeatWave makes ample use of data compression. And it’s applying some common techniques such as Bloom filters that reduce the amount of intermediate memory required for query processing. Specifically, HeatWave has implemented Blocked Bloom filters that can perform the necessary data lookups with much less overhead, significantly reducing the amount of intermediate memory required.  These capabilities, in turn, clear the way for Oracle to introduce the capability to process machine learning models inside the database, without need for an external ETL engine or a machine learning execution environment. And in so doing, Oracle is following a trend that has also seen in AWS (Amazon Redshift ML), Google (BigQuery ML), Microsoft (SQL Server with in-database R and Python functions), Snowflake (with Snowpark), and Teradata (via extended SQL). But comparing these approaches is like comparing apples and oranges, as each provider takes different paths ranging from developing models externally to providing limited, curated choices for running ML, while others extend SQL itself. Heatwave goes the curated route. It’s an approach suited for business analysts or “citizen data scientists” for democratizing machine learning in the same way that self-service visualization placed BI into the hands of the average user. By contrast, the external route is aimed at data scientists in organizations competing on their capability to develop their own unique, highly sophisticated models. A bonus of the curated approach is that it doesn’t require external tools, meaning that selecting, configuring, training and running ML models is performed entirely inside the database. That eliminates the overhead and cost of moving data to tools or ML services running on separate nodes. Oracle also touts the fact that keeping it all in-database reduces potential attack surfaces and consequently reduces security exposure. Here’s how HeatWave’s AutoML approach works. The user chooses the table, columns and the type of algorithm (e.g., regression or classification), and then specifies where the model artifacts should be stored. The system automatically determines the best algorithm, the appropriate features, and the optimal hyperparameters and generates a tuned model.  It streamlines key steps;. For instance, when testing a candidate model, it separates individual tasks or steps that the model performs, with each step evaluated using proxies or stubs that simulate the algorithm against a representative sampling of hyperparameters. It then automatically documents the choice of data, algorithms and hyperparameters to make the model explainable, as shown in the figure below. The advantage of in-database ML processing is a flatter architecture and elimination of the overhead of data movement. While the flipside of bringing in any application processing into the database is heavier processing overhead, there are several design features that make these issues moot.  Cloud-native architecture, which allows compute to be scaled as necessary, eliminates the issue of contention for limited resources. Furthermore, most cloud analytics platforms supporting in-database ML either streamline or support only limited libraries of models to prevent the AI equivalent of the workload from hell, especially for training runs that tend to be the most time-consuming and compute-intensive. Oracle has published ML benchmarks for HeatWave 3.0 that are available on GitHub for customers and prospects to run themselves and verify. Oracle’s introduction of ML processing in HeatWave complements an ML-related feature from its last release, version 2.0 from last summer. That release featured MySQL Autopilot, which uses internalized machine learning to help customers operate the database, such as suggesting how to provision and load the database, while offering closed-loop automation for failure handling/error recovery and query execution.  With version 3.0, MySQL HeatWave comes full circle, using ML to help run the database and support running ML models inside it. This is another example of a prediction I made for this year, that machine learning will take center stage, both for optimizing the operation of the database and for providing customers the capability to develop and/or run models in-database."
17,Databricks vs Snowflake: The race to build one-stop-shop for your data,"March 29, 2022 11:39 AM",https://venturebeat.com/2022/03/29/databricks-vs-snowflake-the-race-to-build-one-stop-shop-for-your-data/,"The heated competition between enterprise data leaders Databricks and Snowflake continued today, after Snowflake doubled down on its core strength: industry partnerships. Snowflake announced it is bringing Amazon.com’s sales channel data directly into customers’ Snowflake data warehouse instances, as part of its new data cloud for the retail industry. And this comes just days after Snowflake launched a data cloud for the health industry. With enterprises large and small racing to build out their data infrastructure, one foundational piece these enterprise companies all need is an easy place to store their data. To address this need, Databricks and Snowflake have emerged as the best one-stop shops for this. They are locked in a duel, espousing different approaches, and having different cultures. In the one corner is Databricks, which innovated what is called a data lake, a place where you can dump all of your data – no matter the format – and was built and is still run by researchers and academics who dream of “changing the world,” says the company’s CEO Ali Ghodsi, who was an academic for seven years before founding Databricks. It’s tech focused, and engineering-led. In the other corner is Snowflake, which innovated what is called the data warehouse, a place that, simply put, starts with more structure, to allow more easy analytics on the data. And it’s run not by researchers and academics, but a CEO Frank Slootman, who’s had more than a decade of experience as a business executive running large companies as CEO or president. And now, while they come from different ends of the spectrum, they are now branching out into each other’s territory, with the goal to build the one-stop-shop for all things enterprise data or what many refer to as a ‘lakehouse’. Their recent moves continue to show how different they are. “Snowflake’s innovation is its investment in its ecosystem and partnerships – and its PR and sales machines,” said Andrew Brust, founder of strategy and advisory firm Blue Badge Insights. “They are great sellers and are building a data marketplace that adds real value. On the other hand, Databricks is very focused on technological excellence, performance, features and high-end machine learning capabilities.” Cloud data lakes and warehouses have become a critical element in answering enterprise data management needs. Organizations typically take enterprise data from various sources and operational processes, and first store it in a raw data lake. Then they can perform a round of ETL (extract, transform, load) procedures to shift critical parts of this data into a form that can be stored in a data warehouse. This is where business and other users can more easily generate useful business insights from the data. While the process has been useful, companies often find it difficult and costly to maintain consistency between their data lake and their data warehouse infrastructures. Their teams need to employ continuous data engineering tactics to ETL/ELT data between the two systems, which can affect the overall quality of the data. Plus, because the data is constantly changing (depending on the pipeline), the information stored in a warehouse may not be as current as that in a data lake.  That is why Databricks has been working hard to make its data lake more compatible with the features of a warehouse, and Snowflake has been adapting its warehouse to allow more features of a data lake. Both really now look like lakehouses. While the data industry has seen and continues to see many data platforms, including offerings from Amazon and Google and a bunch of startups, Databricks and Snowflake have left a particular mark.  To understand, we have to go back to Hadoop. Nearly two decades ago, the open source Java-based framework took the initial steps to solve the storage and processing layer for big data, but it failed to gain widespread adoption due to technical complexities.  Snowflake, founded in 2012 by former Oracle data architects Benoit Dageville and Thierry Cruanes, came to the scene as a better, faster alternative to Hadoop. In no time, the company became the go-to choice for a cloud database that would give customers a single platform to store, access, analyze and share large amounts of structured data from anywhere (AWS, Azure, or any other source). It transformed the warehousing space by offering highly scalable and distributed computation capability. Today, Snowflake customers can easily connect business intelligence tools such as Tableau and conduct historical data analyses using SQL on their datasets. The ease of use and scale of the platform has driven massive adoption of Snowflake over the years. It went public in 2020, and rocketed to a market value of $100 billion, as the pandemic pushed enterprises to invest more into their data infrastructure to allow for things like hybrid work. Its value has since come down to around $73 billion (as of March 29). The revenue of the company has grown 106% from $592 million in FY21 to $1219 million in FY22, while the customer base has surged to over 5900 – including about two-fifths of Fortune 500 companies. Databricks, meanwhile, was founded in 2013, although the groundwork for it was laid way before in 2009 with the open source Apache Spark project – a multi-language engine for data engineering, data science, and machine learning. Spark drew widespread attention with its in-memory processing, which allowed faster and more efficient handling of workloads. So, the team at the backend, academics at UC Berkeley, commercialized the project by founding Databricks, which offered enterprises a cloud SaaS platform (data lake) primarily aimed at storing and processing large amounts of unstructured data for training AI/ML applications for predictive analytics.  Since then, the company has roped in over 5000 enterprises as customers, including Condé Nast, H&M Group, and ABN AMRO. It has also raised significant capital, with the most recent round of $1.6 billion in August of 2021 valuing the company at $38 billion. It is still not public but clearly continues to gain traction.  Initially, Databricks and Snowflake stayed clear of each other, focusing on growing in their respective markets: Snowflake was building the best data warehouse and Databricks was building the best data lake. Ali Ghodsi, the CEO of Databricks and an adjunct professor at UC Berkeley, worked with his fellow co-founders (who were also academics) and developed a largely engineering and product-driven culture for the data science community. “We took three key bets: 100% cloud, open source and machine learning. I think we are today the largest commercial open source-only vendor that’s independent. That has a lot to do with because we were looking far into the future,” Ghodsi told Venturebeat.  “As academics and researchers, you think about how you can change the world while as a business person you look at how much money you can make this year or next year or what Wall Street will say in the next three years,” he said.  Snowflake, however, took its early steps in the market under the leadership of a strong product leader, Bob Muglia. A veteran from Microsoft and Juniper Networks, Muglia joined as CEO two years after Snowflake was founded. However, in 2019, he stepped down, and Frank Slootman, an experienced business executive who had led companies for a decade and a half, took over. With a master’s degree in economics and the experience of successfully leading three tech giants to IPO (including Snowflake), Slootman has promoted a sales-driven culture at the company with aggressive business execution through partnerships and marketing. “He is a commercial pro that takes companies from zero to 60 in three seconds. Doesn’t really matter what the company is. For Ali, on the other hand, this is his life’s work. He’s a scientist,” said a senior executive at a company that does extensive business with both companies, but who requested anonymity to avoid offending either company. Snowflake’s customer-focused approach was also reiterated by Christian Kleinerman, the company’s SVP of Product. According to him, it began from the early days of the company under Muglia who shaped the culture of the company. “His fingerprints are in many areas of what we do. We’ve been public in aspects like our business model, how we think about customers, the obsession with customers, which to this day is a key value. We’re honest about it, spend all of our time on the needs of our customers, not anything else, not intellectual stimulation of interesting problems or competitors. Now it’s all customers. Of course, our founders play a big role in that as well. But, Bob was a big part of that,” Kleinerman said. Now, after building successful businesses in different corners of the data space and on the back of these very different cultures, Snowflake and Databricks are on a collision course. Databricks has been moving towards offering the capabilities and performance of a data warehouse with its core artificial intelligence (AI) offering, while Snowflake has been inching towards adding data science workloads (among other things). Databricks is marketing its SQL analytics and business intelligence (BI) tool integrations (like Tableau or Microsoft Power BI) for structured data by using the term lakehouse more widely. Meanwhile, Snowflake has launched new data lake-like features, including support for unstructured data and the ability to build AI/ML projects. Although, instead of lakehouse, it is using the term “Data Cloud” to define its broader, more comprehensive offering. “We think first about data structure and data governance… that’s where we start. And now, our vectors of expansion are how do you bring more capability into our platform without compromising the promise that we have for customers? For us, AI and [machine learning] ML is one such expansion,” Kleinerman told VentureBeat. “We have plenty of customers leveraging us for data transformation and what you would hear is, I don’t want to have to copy my data out to another system and compromise my governance just to do machine learning training. So that’s what we’re bringing in,” he said. In addition to this, both companies have also debuted dedicated vertical-specific offerings to cater to retail, healthcare and other growing sectors. While Ghodsi cited a UC Berkeley study to note that adding machine learning algorithms on top of a data warehouse is basically a ‘large hack’ and not very feasible due to performance and support issues, Snowflake claims otherwise. “Data is the most important part of any ML system, and Snowflake is continually innovating in how to mobilize the world’s data to best enable ML. Snowflake was designed from the ground up to be a single source of data truth, and to deliver the performance, speed, and elasticity needed to process the growing volume of data that powers ML workflows,” Tal Shaked, ML Architect at Snowflake, told VentureBeat. The two companies have also been engaged in a PR battle, with Databricks claiming that its SQL lakehouse platform provides superior performance and price-performance over Snowflake, even on data warehousing workloads (TPC-DS), while the latter disputing it blatantly. On November 2, Databricks shared a third-party benchmark by the Barcelona Supercomputing Center to note that its SQL lakehouse performs 2.7 times faster than a similarly sized Snowflake setup (which took 8397 seconds). However, about ten days later, Snowflake published a blog saying that the claim lacks integrity and is wildly incongruent with its internal benchmarks and customer experiences. Instead, it claimed to have run the same benchmark in 3,760 seconds. The company even asked users to test for themselves. In response, Databricks suggested that the improved performance was the result of Snowflake’s pre-baked TPC-DS dataset, which had been created two days after the announcement of the results. With the official TPC-DS dataset, the performance was nowhere near what Snowflake claimed. While it remains to be seen who comes out on top in the performance and price-performance battle, in the long run, the general expansion of Snowflake and Databricks’ capabilities is a good sign for the overall industry. As enterprises become heavily reliant on data for growth and innovation, these two players – with their BI and AI capabilities – will be critical in ensuring that data teams could choose a single platform for data management, without taking the burden of handling two separate systems."
18,Computing’s new logic: The distributed data cloud,"March 29, 2022 11:07 AM",https://venturebeat.com/2022/03/29/computings-new-logic-the-distributed-data-cloud/,"A common pattern in analytic ecosystems today sees data produced in different areas of the business pushed to a central location. The data flows into data lakes and is cordoned in data warehouses, managed by IT personnel. The original producers of the data, often subject-matter experts within the business domain, effectively lose control or become layers removed from data meaningful to their work. This separation diminishes the data’s value over time, with data diverted away from its business consumers. Imagine a new model that flips this ecosystem on its head by breaking down barriers and applying common standards everywhere.  Consider an analytics stack that could be deployed within a business domain; it remains there, owned by team members in that business domain, but centrally operated and supported by IT. What if all data products generated there were completely managed within that domain? What if other business teams could simply subscribe to those data products, or get API access to them? An organizational pattern —data mesh — that promotes this decentralization of data product ownership has received a great deal of attention recently. However, what ecosystem architectures are well suited to providing the technical backbone for enabling a data mesh, and can deal with the emerging patterns of data growth? As data volumes grow, the idea of moving data to a centralized location for processing becomes more expensive and time-consuming — particularly if that data is generated outside a traditional data center or public cloud. Instead, enterprises will increasingly prefer to deploy analytics processing to the places where the data is generated. The ability to easily geolocate data for latency, compliance or security reasons will transform the way we compute to a more sustainable, efficient, and logical reality—that is the territory of the distributed data cloud. Seamlessly controlling data anywhere is how enterprises take advantage of the incredible data growth that’s upon us.  The distributed data cloud is not a single tool or platform, but an ecosystem pattern that gets data to the right place and the right person at the right time in a secure, governed, and trusted way. It includes a federated collection of data management and analytics services spanning public clouds, private clouds, and the network edge.  Managed from a single control plane, a distributed data cloud enables analytic applications to be provisioned at the point of need on a right-sized blend of physical and virtualized infrastructure, based on data gravity, data sovereignty, data governance, and latency requirements.  Several major trends will drive businesses to embrace the full value of their data with this model, where infrastructure functions to democratize data, not imprison it.    It’s reliably predicted that by 2025, 75% of enterprise-generated data will be created and processed outside the traditional centralized data center or cloud, up from less than 10% in 2019. The explosion of data and devices at the edge and the rollout of 5G and planning for 6G — 100 Gbps networks over the next 10 years — has hastened the realization that the internet backbone doesn’t have enough capacity to backhaul all of the data activity at the edge over to centralized data centers for analysis.    The Gartner Top Strategic Technology Trends for 2021 report suggests that the distributed cloud—the necessary infrastructure as a service precursor of a distributed data cloud platform implementation explained in this article—is emerging to address location-impacted latency. The deployment of cloud software and hardware stacks outside a public cloud provider’s data center to provide a mesh of interconnected cloud resources is what’s meant by distributed cloud. Its stacks allow businesses to run applications developed for the public cloud in a company’s own data center and other locations, like multi-access edge computing centers connected to 5G cell tower groups, or on the factory floor in support of IoT applications in manufacturing. But enterprises still benefit from the value proposition of public cloud and guaranteed SLAs. Both hybrid cloud and hybrid IT break the fundamental value propositions of cloud. Namely, hybrid is very difficult to execute efficiently, fully leveraging the scale and elasticity of services that public cloud offers. Hybrid does not yield efficiencies in cloud operations, governance, and updates that public cloud offers, nor do these systems keep pace with innovation in public clouds. Distributed cloud means the same, seamless cloud experience everywhere.  Enterprises ultimately want to put interactive, predictive analytics in the hands of the actual consumer. To that end, rather than data warehouses serving a user community of a thousand, data warehouses ultimately will serve a user community of millions of end consumers. The current ubiquity of mobile device usage gives an idea of where multi-sensory, multi-device, multi-touchpoint enterprise experiences with data are heading. The computer is fast becoming the environment around the user.   An increasingly API-driven culture everywhere, seamless UX/UI, and democratized data access throughout enterprises will power the shift toward hyper-personalized, real-time interactions among people, places, and things. With these trends spurring the advent of the distributed data cloud, several use cases are on the immediate horizon.   First, there’s a widespread need for simplified hybrid and multi-cloud operations that feature a consistent environment in the public cloud, on-premises, and at the edge. A compelling reason for this, particularly in regulated industries such as banking, is to help reduce cloud concentration risk by distributing data and analytics across more than one cloud provider or data center. To achieve this using a distributed data cloud, an enterprise can provision containerized data management and analytics applications and run them anywhere that Kubernetes is deployed — in a public cloud, on-premises, or at the edge. Everything happens via the same management UX and devops processes and from the same web console and APIs. Second, processing personal identifiable information (PII) in a country of residence is a scenario where localized access and regulatory compliance make moving compute to the data the best solution. Running an instance optimized for distributed data cloud in individual hospitals on a public cloud stack co-located with the hospital allows patient data to remain at the source. A third use case where the need is already skyrocketing involves IoT analytics. The ability to perform secure analytics at the network edge and close to consumers via a distributed data cloud means real-time answers for connected cars, smart cities, energy grids, and much more. Running optimized analytics on AWS Wavelength, for example, in a multi-access edge environment to monitor network quality in real time will be entirely doable. Bringing to life a distributed data cloud — where data anywhere is easily managed and put to work — is not a single-vendor play and it probably never will be. Rather, a consortium of companies that come together around this idea and work symbiotically will bring the party to the data and success to businesses ready to grasp a more logical future. Mark Cusack is the CTO at Yellowbrick"
19,Don’t take data for granted,"March 29, 2022 8:38 AM",https://venturebeat.com/2022/03/29/dont-take-data-for-granted/,"We’ve long had a running joke that the world was running out of data. It’s certainly the type of statement that gets a rise. But it could be argued that, after the emergence of big data over a decade ago, data eventually subsided in the headlines, in favor of artificial intelligence (AI), cloud and microservices. With the cloud making it almost trivial to pile up those terabytes of object storage and turn compute cores on and off with short notice, it’s tempting to wonder if we’re starting to take data for granted. Data matters more than ever. It’s taken for granted that the so-called Three V’s of big data are no longer exceptional. Big data is so 2014 — in the 2020s, we just term it “data.” And data is coming from more sources and places. That’s led to a chicken-and-egg scenario as distributed databases grow more commonplace. The cloud enables it, and the use cases for global deployment demand it. And, by the way, did we mention the edge? In many cases, that data is not going anywhere, and processing must come to it. There is no silver bullet to extending data processing to the edge. Moving to the edge means pushing down a lot of intelligence because there won’t be enough bandwidth to bring in the torrents of data, much of it low density (e.g., instrument readings) where the value only comes from aggregation. And at the back end, or shall we say the hub (in a distributed environment, multiple hubs), it will prompt the need to converge real-time data (e.g., streaming, data in motion) with historical data (e.g., data at rest). That’s been a dream since the early days of what we used to call big data where the only practical solution at the time was the Lambda architecture – separating the real-time and batch tiers. As a result, streaming typically required separate platforms, where the results would be ingested into the database or data lake. That was a complex architecture requiring multiple tools, lots of data movement and then additional steps to merge results. Thanks to the emergence of cloud-native architecture, where we containerize, deploy microservices, and separate the data and compute tiers, we now bring all that together and lose the complexity. Dedicate some nodes as Kafka sinks, generate change data captures feed on other nodes, and persisted data on other nodes, and it’s all under a single umbrella on the same physical or virtual cluster. And so as data goes global, we have to worry about governing it. Increasingly, there are mandates for keeping data inside the country of origin, and depending on the jurisdiction, varying rights of privacy and requirements for data retention. Indirectly, restrictions on data movement across national boundaries is prompting the question of hybrid cloud. There are other rationales for data gravity, especially with established back office systems managing financial and customer records, where the interdependencies between legacy applications may render it impractical to move data into a public cloud. Those well-entrenched ERP systems and the like represent the final frontier for cloud adoption. So, on-premises data centers aren’t going away any time soon, but increasingly, as is HPE’s motto, the cloud may come to you. The draw is the operational simplicity and flexibility of having a common control plane and on-demand pricing model associated with public clouds. That’s why, ushering in the new decade, we forecast that the 2020s would become the era of the Hybrid Default. It’s why the enterprise spinoff of HPE has seen its on-demand hybrid/private cloud business more than double year over year. Demand for the cloud is not a zero-sum game; growing demand for hybrid cloud or private cloud is not coming at the expense of public cloud. And that’s where things get crazy, as cloud providers have built an increasingly bewildering array of choices. When we last counted, AWS had well over 250 services, and looking at the data and analytics lane, there are 16 databases and 30 machine learning (ML)services. The burden is on the customer to put the pieces together, such as when they use a service like Redshift or BigQuery and want to run data pipelines for ingesting and transforming data in motion, visualization for providing ad hoc analytics, and of course, advanced machine learning.  Help is on the way. For instance, you can now in some cases run ML models inside Redshift or BigQuery, and you can reach out to other AWS or Google databases for federated query. Azure, for its part, has strived for more of an end-to-end service with Synapse, where the pieces are built in or activated with a single click. But these are just opening shots – cloud providers, and hopefully with an ecosystem of partners, need to put more of the pieces together. In all this, we’ve so far skipped over what’s been one of the liveliest topics over the past year: the discussion around data meshes. They arose as a response to the shortcomings of data lakes – namely that it’s all too easy for data to get lost or buried, and that the teams who consume the data should take active ownership[ over it. Against that are concerns that such practices may not scale or erect yet new data silos. And so with all this as backdrop, we’re jazzed to start setting up residency here at VentureBeat in the Data Pipeline, along with fellow partners in crime Andrew Brust and Hyoun Park. Hang on, we’re in for some ride."
20,Snowflake readies Amazon retail data for demand forecasting,"March 29, 2022 6:00 AM",https://venturebeat.com/2022/03/29/snowflake-readies-amazon-retail-data-for-demand-forecasting/,"Cloud data warehouse darling Snowflake has been investing a lot in its data ecosystem. Yesterday, the company made a broad announcement around the launch of its Retail Data Cloud, which integrates the core Snowflake platform with partner-delivered solutions and industry-specific datasets. Today, the company is announcing an important initiative within that rollout, which it shared exclusively with VentureBeat: it’s partnering with Amazon Web Services (AWS) to bring Amazon.com sales channel data directly into customers’ Snowflake data warehouse instances. This is a data-driven age for retail, a phenomenon accelerated by the pandemic and the twists and turns COVID has created in buyer preferences and purchasing patterns. The provision of the Amazon.com data is intended to help retail and CPG (consumer packaged goods) customers not only monitor Amazon.com Vendor Central PO data, but also leverage Amazon Forecast functionality within their Snowflake environments. The offering enables sophisticated demand forecasting and helps manage upstream logistics, including lead times for manufacturing and delivery. The companies also explain the data enables customers to track product weight or dimension changes; reduce on-time in-full (OTIF) penalties/improve OTIF metrics; and ensure high-sales SKUs (items) stay in-stock. Such analyses can help companies mitigate the impact of supply chain irregularities and improve the all-important customer experience. As the folks from Snowflake and AWS tell the story, this initiative is about way more than raw data feeds. Rather, it provides a collection of high-value data sets that have already gone through significant data integration processes to help customers avoid pulling and blending data from multiple accounts.  The data also goes through significant data engineering processes that make it analysis-ready right out of the gate. The result: retail brands and CPG businesses get purchase order history and updates, down to the line item level, as well as granular product catalog metadata, providing the ability to perform probabilistic PO forecasting at the ASIN (Amazon Standard Identification Number) or fulfillment center level. This all goes beyond abstract concepts expressed in a press release. In fact, in an exclusive briefing with VentureBeat, Rosemary Hua, global head of Retail & CPG at Snowflake, and Justin Honaman, head of Worldwide Consumer Packaged Goods at AWS, showed how BI tools could be used directly against the Amazon retail data. Creating visualizations of demand against current stock levels, or top-selling product forecast shortage by geographical region (partially shown in the figure at the top of this post), become fairly straightforward, right from a major BI tool (Tableau, in the case of screenshots shown to us). If you’re interested in the mechanics of all this, they’re remarkably straightforward. Data is inserted directly into customer Amazon S3 buckets. From there, Snowflake can detect the presence of the data, determine its schema and automatically refresh the latter as necessary. Customers then have the ability to use and analyze the data in a standalone fashion or join it to data from other, third-party sources in the Snowflake Data Marketplace. When it comes to looking at the cloud data warehouse market, it’s fairly easy to focus on which competitor has the most born-in-the-cloud solution, the best enterprise-class performance and pedigree, or the smoothest combination of the two. But maybe the contest between those sets of criteria misses the point. The technology, including its performance and features, is critical, of course. But so too is the partner ecosystem and, yes, the data. Enterprise application companies like SAP and Oracle appreciate that, offering analytics in the context of transactional data. Snowflake seems to see that value too. Perhaps, as an independent cloud data warehouse provider, emphasizing its partner and data ecosystem is Snowflake’s biggest innovation. If we look past the core platforms themselves, and take seriously the notion of what a “data cloud” really means, then maybe the metrics and dimensions of the cloud data warehouse market start to change."
21,Snowflake launches data cloud for retail industry,"March 28, 2022 6:00 AM",https://venturebeat.com/2022/03/28/snowflake-launches-data-cloud-for-retail-industry/,"In another move to help enterprises better mobilize their data, Snowflake has announced the launch of data cloud for retail. The offering comes days after the launch of healthcare and life sciences (HCLS) data cloud and aims to serve as a dedicated platform to address the challenges retail industry stakeholders – retailers, manufacturers, distributors and consumer packaged goods (CPG) vendors – face while trying to extract value from their datasets.  “The way that consumers, retailers and brands interact is quickly changing, and the data that organizations need to understand and adapt is not as available or easy to work with as it needs to be. Whether a business is working to deliver more personalized purchase experiences or adjusting amid supply chain, shortage, or inflation challenges – among others – the needed data is spread across internal and external stakeholder,” Rosemary Hua, Snowflake’s GTM lead for retail and CPG industries, told VentureBeat. The new Retail Data Cloud solves this challenge by bringing together industry-specific datasets and various partner solutions on a single platform. This way, it drives industry-wide collaboration, enabling the stakeholders to not only access their own data (which can be siloed due to legacy systems) but also new information from other, external sources.  “Our new Retail Data Cloud breaks down data silos within and across organizations and offers retail organizations the specific AI and analytics tools they need to make quick, insight-driven decisions to overcome obstacles and adapt to trends across functions – from manufacturing to logistics, marketing, merchandising and beyond. In short, regardless of where precisely in the retail ecosystem an organization lives, we’re making more data more available and easier to work with so they can optimize every part of their business,” Hua said. Snowflake notes that enterprises using the Retail Data Cloud can share data across the ecosystem in real-time across all major public cloud platforms. To ensure strict data governance and regulatory compliance, the company employs a suite of easily managed security capabilities, including data clean rooms, double-blind joins, restricted queries, centralized RBAC (role-based access control) and row/columnar level obfuscation that enables data to be shared without movement and risk of revealing personally identifiable information. On the partner solutions’ side, the Retail Data Cloud provides enterprises with applications/products, curated by Snowflake’s technology, consulting and data partners, to solve industry-specific problems and reduce the time to value. “We are announcing new joint solutions that are being brought to the market starting Monday, March 28th,” Hua said. “One such example is AWS bringing Vendor Central and improved PO forecasting data onto Snowflake for consumer product brands that sell through Amazon.com’s marketplace. Another example is Numerator (Snowflake data marketplace partner) bringing a new product called Secure Enrich, which is built on Snowflake data clean room. Retailers and brands will be able to consume and join these new datasets directly in their native Snowflake environment, making it seamless to manage inventory and enrich customer data,” she added. Currently, the Retail Data Cloud is being used by players such as 84.51°, Albertsons, Kraft Heinz and Rakuten. Overall, Snowflake claims more than 1,000 retail and CPG companies have signed up for its platform, which supports six key workloads – data engineering, data sharing, data warehousing, data application development, data lake and data science. Notably, a similar offering has also been curated by Databricks and Dremio as part of what they call the data lakehouse approach. Databricks, in particular, has been strengthening its offering with partner integrations and vertical-specific offerings. The company was last valued at $38 billion, while Snowflake’s current (as of March 25) market capitalization stands at about $66 billion."
22,Synthetic data and the Wells Fargo-Hazy relationship,"March 28, 2022 6:00 AM",https://venturebeat.com/2022/03/28/synthetic-data-and-the-wells-fargo-hazy-relationship/,"Over the years, legacy financial institutions have been generating and sitting on a wealth of useful data. Unfortunately, strict privacy and security controls have limited how these institutions can use their own data. Such constraints are a problem because they block innovation.  To keep market share and stay ahead of the pack, legacy financial institutions need to figure out an alternative. Synthetic data is emerging as the solution to financial institutions hamstrung by silos and governance protocols.  A synthetic dataset preserves the statistical properties of its real-life equivalent but loses real information that can compromise privacy. As a result, synthetic data finds use in studying trends and anomaly detection, which are a few bread-and-butter use cases of machine learning (ML) algorithms. Harry Keen, CEO of synthetic data-generating platform Hazy, points out that creating a synthetic dataset involves training a generative model to “learn all the statistical characteristics in the raw data and generate row upon row of fictitious data points.”  “You can use it exactly the same way as the real dataset in many use cases across the enterprise,” Keen said. And that is a golden ticket that financial institutions are banking on. Wells Fargo, for example, has added Hazy to its startup accelerator program, its way of bringing in early-stage growth companies who can solve problems that the financial institution is looking to address. While Hazy is by no means the only synthetic data startup, it caught the attention of Wells Fargo for many reasons, including its targeted niche in financial services. Another factor in Hazy’s favor: the self-service model for generating and using datasets so Wells Fargo data scientists can easily request and use the kinds of synthetic datasets they need. “We wanted to make sure that it would be extremely easy for our data scientists to access data and train models,” said Madhu Narasimhan, head of innovation and strategy, digital & innovation COO with Wells Fargo. “Absent that, we would just be transferring manual labor from one place to the other; we don’t harness any real value,” Narasimhan said. Another advantage: “Synthetic data allows us to carry out our experiments at scale.”  For now, Hazy and Wells Fargo are focused on laying the groundwork for synthetic datasets. “Our first step is to get out of the business of curating, sourcing, labeling, and the time-consuming work of just prepping the data,” Narasimhan said. “We want to get into the more intelligent use cases of tooling it.”  The intelligent use cases Wells Fargo has in mind for the synthetic data it will harvest from Hazy, include fraud detection using machine learning models. Once synthetic datasets take a first pass at replicating the real-life equivalent, they can be fine-tuned to amplify different regions of the dataset. For example, you can have a synthetic dataset that has more fraudulent banking transactions than the real data. Building a ML model on the foundation of such datasets makes fraud easier to detect because the model has simply seen more instances of it. When applied back to real data, that model might perform better, Keen said. Synthetic data can also be used to correct one of the biggest challenges in working with legacy institutional information: bias. While the first pass will faithfully reproduce bias prevalent in the original data, subsequent sets can be retrained to include more women in job-hiring algorithms, for example. The challenge, though, is that simply increasing numbers might not do much to scrub related biases – like wage inequities – off the table.  As for Wells Fargo, in addition to the fraud detection use case, Narasimhan is looking forward to improving the customer engagement experience in the future. Narasimhan’s advice to businesses who are looking to complement their operations with synthetic data: “Realize the value of data and then figure out how you’re going to expend that data capital in your business.” Work backwards from your outcomes to figure out if and why you need synthetic data, she said. Narasimhan has been struck by how the excitement related to Wells Fargo’s use of synthetic data. “We underestimated the amount of enthusiasm our internal data scientists would have for this,” she said. “It’s not just a plus for our business and our ability to serve our customers better; it’s a great workforce energizing mechanism as well.”"
23,Datagen nabs $50 million to provide synthetic data for computer vision,"March 23, 2022 5:00 AM",https://venturebeat.com/2022/03/23/datagen-nabs-50-million-to-provide-synthetic-data-for-computer-vision/,"New York-headquartered Datagen has raised $50 million in its series B funding to strengthen its platform and meet the growing demand for synthetic data in the broader AI space. Today, every organization understands an AI model is only as good as the data it is trained upon. Companies give particular focus on sourcing and annotating data correctly, but when it comes to computer vision models, the task becomes twice as difficult. This is largely due to the scarcity of high-quality 2D and 3D visual training data.  A study conducted by Datagen itself found 99% of computer vision (CV) teams have had a machine learning (ML) project canceled due to insufficient training data while 100% saw delays due to the same problem. At the core, either they don’t have domain-specific, fair and correctly annotated training data or what they have is just not enough for driving the expected results. Founded in 2018, Datagen solves this problem by providing computer vision teams with a self-service platform to design synthetic datasets. It allows users to create on-demand datasets of people and customize them according to parameters such as ethnicity, gender, environmental interaction and expression. This way, companies not only get training data for their application on a large scale but also with high variance. They can also define how much of the total dataset would be attributed to one particular subject. “Our platform includes a range of tools and generators, including application-specific tools, such as our “in-cabin automotive” solution which is an interface optimized for producing data to train driver monitoring systems (DMS). Datagen Faces Generator, meanwhile, allows the user to control attributes like age, gender, facial expression, gaze direction, as well as scene-specific parameters, such as camera location, and lighting,” Datagen CEO Ofir Zuk (Chakon) told VentureBeat. “With our application-specific solutions, like the aforementioned in-cabin automotive generator, users can control the identity of the subject, and generate that subject playing out certain common DMS scenarios, such as “Falling asleep at the wheel,” or “Using their cellular phone.” For each of these scenarios, the user can generate their range of subjects engaging in these activities in 10-second animated clips – again, with variation around the scene, lighting, camera angle, etc. Once the parameters are set to the user’s liking, the engine then generates a robust, targeted dataset of still images and/or animated clips that can be applied in training,” he explained. Ultimately, the solution enables enterprises to do away from manually sourcing and annotating and switch to a way that provides the required 2D, 3D visual data at scale and ease. Computer vision teams can use it to get to market faster whether they are developing applications for robotics, smart security/monitoring or some other area. “Labeling real-world visual data is not only incredibly time-consuming and resource-intensive, but it’s also a major source of errors and inconsistencies. With Datagen, you’re able to not only skip the time and expense of human annotation but also ensure much higher data quality. Datagen modalities provide accurate annotations for each image — for example, the exact head yaw/pitch/roll, the exact direction of the eye gaze — at levels of detail and accuracy that cannot be achieved with real-life data and manual annotation,” the CEO added. With the fresh round, which was led by Scale Venture Partners, Datagen plans to accelerate growth and strengthen its position as the leading synthetic data provider for computer vision projects. The company’s revenue has grown eightfold YoY since launch and its customer base includes Fortune 100 companies and three of the top five tech giants.  While Ofir did not share the company names, he did note that Datagen is not beholden to a single industry or use-case in the computer vision segment.  “We’ve already seen considerable success with our application-specific offerings, such as our in-cabin automotive solution. Moving forward, we’ll be expanding our human-centric offering to additional domains that cater to our customers needs. The Metaverse will also be an even larger area of focus for us moving forward. As interest and demand continue to outpace development, we see a significant opportunity for synthetic data to serve as a significant enabler of the Metaverse. Lastly, we’re actively developing additional tools and solutions on top of data Generation, with the goal of establishing a comprehensive, streamlined infrastructure for Computer Vision,” he emphasized. Globally, the demand for synthetic data is expected to continue for all AI applications, including computer vision model training. According to Gartner, by 2024, 60% of the data used for the development of analytics and AI projects will be synthetically generated, and by 2030, synthetic data will surpass real data as the preferred tool for training AI models. Other companies operating in the same space include Mostly AI, Rendered AI, YData and Synthetaic. However, Datagen claims to be unique in the sense that it allows CV teams to simulate dynamic humans and objects in their context. They can generate, train, evaluate and repeat to improve the accuracy of their models.  “The Datagen Platform uses proprietary, virtual camera technology so users can ‘photograph’ real-world 3D data in photo-realistic simulations, thus creating hyperrealistic environments and training data. Finally, Datagen’s Zero PII design provides teams with photo-realistic, human training data without any concerns around personally identifiable information (PII),” the CEO said. “By design, Datagen’s product infrastructure supports modularity and expandability of use cases and domains with close to zero overhead. This way Datagen can offer a rapidly increasing number of use cases that cover the growing needs of enterprise customers.”"
24,The state of data in 2022? Decentralization.,"March 22, 2022 6:00 AM",https://venturebeat.com/2022/03/22/the-state-of-data-in-2022-decentralization/,"Anyone who’s anyone knows that data is one of the world’s greatest resources, but businesses face significant challenges as they seek to unlock the benefits of data that’s spread across myriad systems. That’s according to a new report commissioned by Red Hat and Starburst, which highlights some barriers companies face accessing data as it’s generated in real time. The second annual The State of Data and What’s Next report, which includes input from some 400 companies from across the geographic and industrial spectrum, points to the growing shift from a centralized to decentralized data infrastructure, with companies now averaging four to six distinct data platforms — and some as many as 12. This is roughly in line with last year’s data, which found that 52% of respondents had five or more different data platforms in their ecosystem. There are many reasons why the number of disparate data platforms within a company are growing — for starters, it’s now easier than ever to spin up a new data store, thanks to the proliferation of the cloud. “Since 2016, cloud technologies have made it very easy and reasonably cheap to spin up new data stores,” Starburst VP of data mesh Adrian Estala told VentureBeat. “‘Storage is cheap’ was a common phrase — you could literally spin up a new environment in days, with a credit card.” On top of that, there is simply more data than what companies know what to do with, which has inevitably led to a gargantuan data sprawl. “From IoT to sensors and mobile devices, we suddenly had more data than we ever imagined,” Estala continued. “Forbes had a famous quote that 90% of the data had been created in the last two years, [but] we probably created that much in a month this year. If data is the new oil, then what took 50 million years to create (oil), now takes a month (data).” For context, a “data platform” could be anything from an analytics system or data lake, to a data warehouse or object storage. The more such platforms a company has in its IT set up, the more complexities there are in terms of unlocking big data insights. This is particularly true for so-called “streaming” data, which is concerned with harnessing data in real time — this can be useful if a company wants to generate insights into sales as they’re happening, for example. When asked what types of new data they planned to collect in the next year, 65% of respondents cited streaming data as their top priority, which was followed by video and event data, which were tied on 60%. Elsewhere, the report found that around half of the companies surveyed take more than 24 hours to create a new data pipeline to move and transform data between locations, and then a further 24 hours (at least) to operationalize the pipeline and deploy it into a production setting. This was identified as one of the major problems that companies face as they strive for real-time business insights, and is partly why the industry is moving away from the pipeline process toward a decentralized model — or a “data mesh,” as it’s often called today. This data mesh basically makes data available to anyone, anywhere in a company, with a focus on speed — being able to access the data at its source, rather than having to transport and centralize it. The report showed that while the rate of change varies by region, companies by and large are planning a more decentralized data architecture strategy in the coming months. And this, according to Estala, was one of the biggest single surprises they saw in this year’s report — the speed at which organizations have pursued decentralization. “The shift to a decentralized model happened very, very fast,” Estala said. “Just a year ago, we were having difficult arguments on the best way forward — the big cloud providers that many organizations were ‘hitched’ to were adamant that centralization was the only way. This shift [to decentralized] is business-driven, not IT-driven. This demonstrates the urgency to deliver digital transformation. IT has realized that we can’t migrate to — or sustain — a centralized architecture with the efficiency that the business demands.” Ultimately, companies are starting to prioritize faster data access, and this is partly in response to the pandemic-driven challenges of the past couple of years. The report noted that supporting customer engagement was the most common driving force behind their push toward real-time data and analytics (33%), which was followed by a desire to stay ahead of risk and marketing swings (29%) and employee engagement (29%). Other notable trends to emerge from the report include the great migration toward the cloud, with respondents noting that 59% of their data is now stored in the cloud vs. 41% on-premises, up from the 56% vs. 44% that emerged in last year’s report. Aside from highlighting the growing prominence of cloud computing, this also serves as a timely reminder that multi-cloud and hybrid models remain a popular alternative for companies that are unwilling or unable to make the full transition. Indeed, “multi-cloud flexibility” was cited as the top (43%) influencing factor in respondents’ buying decisions regarding cloud data storage, with “hybrid interoperability” jumping from 26% to 34% on last year’s report. “Multi-cloud was not where we thought we would end up when we were designing cloud strategies seven years ago, but it is now a reality,” Estala said. “This, more than anything else, underscores why a decentralized approach like data mesh is the only way forward.” The 2022 State of Data and What’s Next report is available to download now."
25,Real-time data tracking platform Applied XL nabs $3.5M,"March 21, 2022 10:30 AM",https://venturebeat.com/2022/03/21/real-time-data-tracking-platform-applied-xl-nabs-3-5m/,"New York-headquartered Applied XL, a startup that focuses on building algorithms to help enterprises with real-time data tracking, today announced it has raised $3.5 million in seed funding. Companies across industries, regardless of the size, remain flooded with information. Some data is relevant, but a large chunk of it is nothing but unnecessary noise and clutter. As a result, businesses often end up struggling with the problem of filtering and finding valuable insights.  For instance, in the clinical trials space, as many as 1.5 million developments took place this year alone but only 8% of it attributed to relevant events that mattered to enterprises that need to track the space for business/product development. Founded in 2020, Applied XL solves this challenge by leveraging machine learning and editorial algorithms to do the job of humans. The solution, as the company explains, monitors dozens of databases in real-time through a journalistic lens (computational journalism) and tries to make sense of disparate data streams to flag events that could be relevant today or sometime in the future.  “The engine is agnostic to third-party data sources. It includes enrichment layers that dynamically add editorial parameters, machine learning classification, data unification and entity normalization,”  Francesco Marconi, CEO of Applied XL, told Venturebeat.  As part of this, rather than taking a snapshot view of data, the engine tracks thousands of daily changes across multiple data sources to find the most crucial updates. Each of these events is then classified and ranked by machine learning models that have been trained with editorial considerations defined by experts. “By deriving and aggregating proprietary events and continuously looking at these new data points longitudinally, event detection algorithms are able to surface trends that provide an up-to-date view of emerging activity that others may miss,” Marconi said. Simultaneously, the company also works with a network of experts who contribute data to retrain the system’s algorithms and improve accuracy. While Applied XL’s goal is to help enterprises measure potential disruptions across the health of people, places and the entire planet, it is currently using it only for the former by providing Trials Pulse – a product developed in partnership with health-focused publication STAT News. Trials Pulse allows pharma, healthcare and biotech professionals to navigate the most important updates happening in the clinical trials space and inform regulatory strategy, clinical development and investment decisions. For instance, the company notes, the platform helps identify patterns in patient enrollment changes and timeline shifts to anticipate interruptions to clinical trials. “Applied XL has already been used by 700+ users spanning across global pharmaceutical companies, emerging biotech firms and life sciences investment funds,” Marconi said. “They are using our platform to inform decisions on partnerships, competitive positioning, regulatory strategy, business development and investment considerations.” The CEO did not share the names of the customers, but he did note that a number of them are upgrading to their annual subscription priced at $1500. With this round, he said, the company will accelerate the development of Trials Pulse and expand its coverage in the healthcare and life sciences industry. A part of the funding, which was led by Hearst Ventures, will also go toward hiring fresh talent. Applied XL could play a major role in better tackling challenges such as pandemics, infrastructure issues and the effects of climate change. However, it is far from being the only one exploring the real-time data tracking space. Companies such as Diffbot, Dataminr and AlphaSense are also active in the same space. Plus, in the financial services, Dow Jones and Bloomberg have made a significant mark. Marconi indicated Applied XL differentiates itself by diving deep into certain areas of certain underserved industries, such as life sciences.  “Professionals in pharma and biotech (for instance) report being overwhelmed with unstructured clinical trials data and missing out on crucial information, thereby delaying their decision-making. One of the drivers for this is that a substantial amount of clinical trials and drug regulation data is unstructured. That’s where Applied XL comes in,” he said. Globally, the life science analytics market is projected to reach $42 billion by 2025."
26,Snowflake launches data cloud for healthcare and life sciences,"March 17, 2022 6:00 AM",https://venturebeat.com/2022/03/17/snowflake-launches-data-cloud-for-healthcare-and-life-sciences/,"Bozeman, Montana-headquartered data company Snowflake has expanded its “data cloud” ecosystem with the launch of a dedicated offering for healthcare and life sciences industries. Officially dubbed Healthcare & Life Sciences (HCLS) Data Cloud, the product aims to provide enterprises in these sectors with a single cross-cloud data platform to centralize, integrate and exchange critical and sensitive data at scale. It is tailor-made to solve key challenges that keep healthcare enterprises from leveraging data for innovation and continues to be backed by various technology, data, application and consulting partners, including Equifax, Dataiku, H20.ai, Cognizant, Deloitte and Strata. For most healthcare or life sciences companies, the task of leveraging data for medical innovation can be described as one associated with regulatory and technical hurdles. The firms often rely on legacy architectures (that keep data in fragmented siloes) and have to follow stringent compliance rules, with no common models for data sharing with the industry. This makes downstream use difficult, affecting advanced analytics and AI projects, including those for patient care and optimizing clinical and operational decision-making.  Snowflake’s Healthcare & Life Sciences Data Cloud solves these challenges by providing an agile and interoperable solution that facilitates the storage, sharing, and use of data. It eliminates technical and institutional silos while ensuring the security, governance and compliance required to meet industry regulations. In all, Snowflake’s data cloud ecosystem facilitates six key workloads — capabilities of a data warehouse, data lake, data engineering, data sharing, data science and data app development. “The Snowflake Healthcare & Life Sciences Data Cloud will unlock the next generation of innovation in the industry by enabling organizations to take advantage of borderless data access while ensuring strict data governance, security, and privacy compliance,” Todd Crosslin, Healthcare and Life Sciences Industry Principal at Snowflake, said. “The entire industry can benefit from this live, connected ecosystem to get access to the data they need when it’s needed.” The solution is already in use by various organizations, including Anthem, IQVIA, Komodo Health, Siemens Healthineers, Spectrum Health, Novartis and Roche. The launch of Snowflake’s HCLS Data Cloud comes just a week after Databricks’ launch of lakehouse for healthcare and life sciences. The two companies are set out for the same goal – to be the one-stop shop for all things enterprise data.  In fact, both Snowflake and Databricks, led by Ali Ghodsi, have been expanding their product ecosystem to cover more verticals and use cases. The former recently announced the acquisition of Streamlit to simplify data app development, while the latter has debuted lakehouse for retail and financial services, as well as a solution to integrate partner data tools with ease."
27,SentinelOne launches DataSet to manage live data at scale,"March 15, 2022 6:00 AM",https://venturebeat.com/2022/03/15/sentinelone-launches-dataset-to-manage-live-data-at-scale/,"We live in a data-defined era. Distributed cloud infrastructure and containerized applications mean that enormous amounts of data are coming in, and coming in fast.  “The amount of data created in the next three years will be more than the data created over the past 30 years,” said Stephan Elliot, group vice president for Research IT, Cloud Operations, and devops at IDC.  When collected and vetted correctly, data can answer critical questions and bolster enterprise insights. Companies of all sizes need to be able to understand and act on live data sets, in real-time and at scale, without being hampered by legacy data offerings that are expensive, slow, or incapable of keeping real-time pace.  “The ability to cost-effectively analyze data at scale will become a necessity for every organization,” Elliott said.  Log monitoring, also known as log management, is becoming a crucial component to this. It streamlines and enhances the process of storing, transmitting, analyzing, reporting, and acting on the considerable volumes of data generated by an organization’s networks and systems. According to KBV Research, the global log management market will grow to $3.3 billion by 2025, rising at an 11% compound annual growth rate (CAGR). Established providers in the space include Splunk, Databricks, Datadog, Sumo Logic and Dynatrace.  Autonomous cybersecurity platform company SentinelOne is now applying its methods in the security realm to data analytics and log management. The Mountain View, California-based company has announced its new division DataSet, an enterprise data platform enabling live data queries, analytics, insights, and retention, according to CEO Tomer Weingarten. “Every business benefits from the power of understanding its data,” Weingarten said. “Instantaneous, easy to use, and efficient understanding of a data set is the key to making better business decisions and building more sustainable businesses.” The new division builds on SentinelOne’s $155 million acquisition in February 2021 of Scalyr, a cloud-native, cloud-scale data analytics platform. Scalyr’s functionality has allowed SentinelOne to ingest, correlate, search, and action data from any source, Weingarten said.  DataSet is a cloud-native enterprise data platform for all types of data, live or historical, at any scale. Data schema requirements have been eliminated from the ingestion process and index limitations from the querying process. This enables the platform to process massive amounts of live data in real time from both structured and unstructured sources, delivering fast log management, data analytics, and alerts.  Asana, Copart, TomTom and DoorDash have applied DataSet to various use cases, Weingarten said, and enterprise users highlight its real-time detection and response and central management. Previously, these companies had to search and scan multiple tools, or stitch together context across teams and use cases. Neither of these, Weingarten pointed out, are ideal when responding to a critical incident.  DataSet arose out of SentinelOne’s own experiences as a company that collects data on immense scales, he Weingarten. When they began searching for a provider to address limitations and challenges with data analytics, they couldn’t find one that could meet broad or real-time applications. They knew other organizations had to be facing challenges in wrangling and applying data, and that also wanted the capability to do more with their captured data.  “This is unsolved, there’s a gap in the market here,” Weingarten said. “It seems like a pain point for a lot of companies and customers of ours.”  SentinelOne has established itself in the security realm by tackling cybersecurity as a data problem. Its flagship Singularity XDR platform has been built to autonomously defend against security threats with dataset-powered AI models that instantly determine if a behavior is benign or malicious. Individual data points are automatically linked to enable visibility and response. Weingarten underscored the company’s deep data expertise and its abilities to process petabytes of data that continues growing on an exponential scale. And it’s not just getting as close to real-time as possible — it is real-time, he said. Because if you can’t effectively act on data, you’re losing time.  “For cybersecurity to be effective, it has to be able to make split second autonomous decisions,” Weingarten said. “In cybersecurity, every millisecond matters.” Following the release of DataSet, the next logical step, he said, will be applying data to immediate actions across the enterprise. “You don’t want to just be a passive data storer,” Weingarten said. “The goal is to become much more of an active hub.”
SentinelOne is sharing its data expertise “to help all businesses unlock the power of their data,” said Rahul Ravular, head of the new DataSet division. “We help organizations overcome the slow, costly legacy platforms that can’t handle the scalability requirements of tomorrow. DataSet is built for the future of data insights and action.”"
28,Why data fabrics are the next step in the maturation of data management,"March 14, 2022 10:07 AM",https://venturebeat.com/2022/03/14/why-data-fabrics-are-the-next-step-in-the-maturation-of-data-management/,"Navin Sharma, vice president of product at Stardog Gartner indicates that data fabric is the foundation of the modern data management platform, enabling augmented data integration and sharing across heterogeneous data sources. Relying on traditional integration paradigms that involve moving data and manually writing code is the primary reason data scientists and data engineers spend almost 80% of their time wrangling data before any analytics are actually performed. It may also be the reason why Gartner believes that by 2024, data fabric deployments will quadruple efficiency in data utilization while cutting human-driven data management tasks in half. One way to eliminate this burden is by simplifying data integration tasks, reducing data storage costs, and improving cross-domain insights to power downstream analytics. Recently, organizations are discovering solutions that can help, including enterprise knowledge graphs, which have become the centerpiece of a properly implemented data fabric and compound its value for better, faster, lower-cost analytics. The enterprise data landscape is increasingly hybrid, varied, and changing. The emergence of IoT, the rise in unstructured data volume, the increasing relevance of external data sources, and the trend towards hybrid multicloud environments are obstacles to satisfying each new data request. Data fabrics enabled by Enterprise Knowledge Graphs offer a new way forward by weaving together data from internal silos and external sources and creating a network of information to power business applications, AI, and analytics. Quite simply, they support the full breadth of today’s complex, connected enterprise. And it’s their time thanks, in part, to the following trends: Graph technologies were originally created decades ago supporting niche situations, but just as we’ve seen the number of data scientists, engineers, and analysts grow as business’ data needs increased, the technology, too, has scaled, improved, and adapted to new applications and new users. We see wider adoption of these technologies throughout the enterprise as part of a modern data and analytics stack, from applications that support streamlining data operations for companies that build their entire business model around data monetization to large enterprise organizations supporting various cross-functional needs for data uniformity and data linking across the enterprise for faster, richer insights.  The knowledge graph platform provides the glue within this modern data and analytics  stack, operating between the storage layer, the consumption layer, and the data catalog to link all relevant data and metadata to a semantic layer that brings the data to life. This semantic layer enables better data storytelling by attaching meaning and relating similar ideas and providing knowledge of the data supply chain for further context without introducing the complexity of the underlying data structures.  As enterprises are looking to increase the adoption of enterprise knowledge graph technologies, we see that the system integrators (SI), that likewise serve enterprises, are starting to seek out enterprise knowledge graph skills and technologies. These SIs are looking to leverage an enterprise knowledge graph to help their client organizations become data-driven in support of new revenue streams in the digital world. According to a recent study from McKinsey, high-performing organizations are twice as likely to make data accessible across the organization. It is no surprise then that we are seeing growing interest among the leading system integrators to become educated in this type of technology and are willing to invest their time, money, and resources. They recognize this as their opportunity to bring innovation to their clients as companies invest in modernizing their data and analytics stack. We see this interest as one of the early signs of how the market is responding to technology like ours, which is crossing the chasm from early innovators and adopters to the early mainstream majority. In part, the reason for this wide-ranging and emerging interest is because enterprise knowledge graphs are well-suited to operate across clouds, which is where the data and analytics workload is shifting these days, primarily seen as cost-saving measures while leveraging more data. Enterprises too may have concerns about selecting the best cloud configuration for their needs. There is a lot of hype about, and competition between, various cloud providers, which adds pressure and can make things murky. An enterprise knowledge graph-enabled data fabric, on the other hand, provides a lot of choice. You can grab what you need now and start using it, no matter where it is and what adjustments are required for the future. It also enables you to future-proof your investments, minimizing business disruption and operations, should you want to switch the underlying data storage layer or the data consumption layer on top. In the above-mentioned Gartner report, when it comes to the data fabric approach, “One of the most important components is the development of a dynamic, composable, and highly emergent knowledge graph that reflects everything that happens to your data. This core concept in the data fabric enables the other capabilities for dynamic integration and data use case orchestration.” Organizations need to consider a knowledge graph-enabled data fabric to weave together existing data management systems and enrich all connected apps, as they are truly the next step forward in the maturation of the data management space.  Data fabrics powered by enterprise knowledge graph deliver answers via powerful querying capabilities as well. Because it is not a static entity, its “queryable” data layer allows users to answer questions from across data silos, enabling just-in-time analytics. In a data fabric, query happens at the compute layer above the actual storage layer, connecting otherwise disjointed silos and systems. Data flows from source to app and back again, constantly improving the data fabric.  Mark Beyer, Distinguished VP Analyst at Gartner, summed it up nicely when he wrote, “data fabric can be a robust solution to ever-present data management challenges, such as the high-cost and low-value data integration cycles, frequent maintenance of earlier integrations, the rising demand for real-time and event-driven data sharing and more.”  In the end, with an enterprise knowledge graph-powered data fabric, people and algorithms can make better decisions while reducing the likelihood and risk of data misuse or misinterpretation. It helps create a data culture focused on data sharing, versus data control, that provides an opportunity for self-service and self-sufficiency by making data and insight available to all and not just a handful of data specialists. Navin Sharma is the vice president of product at Stardog."
29,"What is a data lake? Definition, benefits, architecture and best practices","March 10, 2022 4:20 PM",https://venturebeat.com/2022/03/10/what-is-a-data-lake-definition-benefits-architecture-and-best-practices/,"A data lake is defined as a centralized and scalable storage repository that holds large volumes of raw big data from multiple sources and systems in its native format.  To understand what a data lake is, consider a data lake as an actual lake, where the water is raw data that flows in from multiple sources of data capture and can then flow out to be used for a range of internal and customer-facing purposes. This is much broader than a data warehouse, which would be more like a household tank, one that stores cleaned water (structured data) but just for use of one particular house and not anything else. Data lakes can be executed using in-house built tools or third-party vendor software and services. According to Markets and Markets, the global data lake software and services market is expected to grow from $7.9 billion in 2019 to $20.1 billion in 2024. A number of vendors are expected to drive this growth, including Databricks, AWS, Dremio, Qubole and MongoDB. Many organizations have even started providing the so-called lakehouse offering, combining the benefits of both data lakes and warehouses through a single product.  Data lakes work on the concept of load first and use later, which means the data stored in the repository doesn’t necessarily have to be used immediately for a specific purpose. It can be dumped as-is and used all together (or in parts) at a later stage as business needs arise. This flexibility, combined with the vast variety and amount of data stored, makes data lakes ideal for data experimentation as well as machine learning and advanced analytics applications within an enterprise. Data lakes use a flat architecture and can have many layers depending on technical and business requirements. No two data lakes are built exactly alike. However, there are some key zones through which the general data flows – Ingestion zone, landing zone, processing zone, refined data zone, and consumption zone. This component, as the name suggests, connects a data lake to external relational and nonrelational sources – such as social media platforms and wearable devices – and loads raw structured, semi-structured, and unstructured data into the platform. Ingestion is performed in batches or in real-time, but it must be noted that a user may need different technologies to ingest different types of data. Currently, all major cloud storage providers offer solutions for low-latency data ingestion. This includes Amazon S3, Amazon Glue, Amazon Kinesis, Amazon Athena, Google Dataflow, Google BigQuery, Azure Data Factory, Azure Databricks, and Azure Functions. Once the ingestion completes, all the data is stored as-is with metadata tags and unique identifiers in the landing zone. As per Gartner, this is usually the largest zone in a data lake today (in terms of volume) and serves as an always-available repository of detailed source data, which can be used/reused for analytic and operational use-cases as and when the need arises. The presence of raw source data also makes this zone an initial playground for data scientists and analysts, who experiment to define the purpose of the data. When the purpose(s) of the data is known, its copies move from landing to the processing stage, where the refinement, optimization, aggregation, and quality standardization takes place by imposing some schemas. This zone makes the data analysis-worthy for various business use cases and reporting needs.  Notably, data copies are moved into this stage to ensure that the original arrival state of the data is preserved in the landing zone for future use. For instance, if new business questions or use cases arise, the source data could be explored and repurposed in different ways, without the bias of previous optimizations. When the data is processed, it moves into the refined data zone, where data scientists and analysts set up their own data science and staging zones to serve as sandboxes for specific analytic projects. Here, they control the processing of the data to repurpose raw data into structures and quality states that could enable analysis or feature engineering. The consumption zone is the last stage of general data flow within a data lake architecture. In this layer, the results and business insights from analytic projects are made available to the targeted users, be it a technical decision-maker or a business analyst, through the analytic consumption tools and SQL and non-SQL query capabilities Unlike data warehouses, which only store processed structured data (organized in rows and columns) for some predefined business intelligence/reporting applications, data lakes bring the potential to store everything with no limits. This could be structured data, semi-structured data, or even unstructured data such as images (.jpg) and videos (.mp4). The benefits of a data lake for enterprise include the following: Over the years, cloud data lake and warehousing architectures have helped enterprises scale their data management efforts while lowering costs. However, the current set-up has some challenges, such as: In order to prevent your data lake from becoming a data swamp, it is recommended to identify your organization’s data goals – the business outcomes – and appoint an internal or external data curator who could assess new sources/datasets and govern what goes into the data lake based on that goal. Clarity on what type of data has to be collected can help an organization dodge the problem of data redundancy, which often skews analytics. All incoming data should be documented as it is ingested into the lake. The documentation usually takes the forms of technical metadata and business metadata, although new forms of documentation are also emerging. Without proper documentation, a data lake deteriorates into a data swamp that is difficult to use, govern, optimize and trust. Users fail to discover the required data. The ingestion process should run as quickly as possible. Eliminating prior data improvements and transformations increase ingestion speed as does adopting new data integration methods for pipelining and orchestration. This would help make the data available as soon as possible after data is created or updated so that some forms of reporting and analytics can operate on it. The main goal of a data lake is to provide detailed source data for data exploration, discovery, and analytics. If an enterprise processes the ingested data with heavy aggregation, standardization, and transformation, then many of the details captured with the original data will get lost, defeating the whole purpose of the data lake. So, an enterprise should make sure to apply data quality remediations in moderation while processing.  Individual data zones in the lake can be organized by creating internal subzones. For instance, a landing zone can have two or more subzones depending on the data source (batch/streaming). Similarly, the data science zone under refined datasets layer can include subzones for analytics sandboxes, data laboratories, test datasets, learning data and training, while the staging zone for data warehousing may have subzones that map to data structures or subject areas in the target data warehouse (e.g., dimensions, metrics and rows for reporting tables and so on). Security has to be maintained across all zones of the data lake, starting from landing to consumption. To ensure this, connect with your vendors and see what they are doing in these four areas — user authentication, user authorization, data-in-motion encryption, and data-at-rest encryption. With these elements, an enterprise can keep its data lake actively and securely managed, without the risk of external or internal leaks (due to misconfigured permissions and other factors)."
30,"What is master data management? Benefits, components, key strategies","March 10, 2022 4:00 PM",https://venturebeat.com/2022/03/10/what-is-master-data-management-benefits-components-key-strategies/,"Modern enterprises have to maintain a number of systems and applications to manage data on a variety of products and hundreds of thousands of customers. This makes data management inherently complex and fragmented.  As critical business data can change from time to time, there can be instances of variance across all of these systems. For instance, if a sales rep adds an incorrect customer address for a new order, the incorrect information will go through multiple systems of the order fulfillment process until some layer, such as the accounting department, introduces a correction. This would leave two data versions for the same order across systems – one correct and the other incorrect.  With thousands of transactions happening every day, acquisitions being made and customers updating their information, the issue is exacerbated multifold, leaving a large chunk of data out of sync and no way of determining which version is correct, incorrect or outdated. Using this data for analysis can lead to incorrect decision-making. This is where master data management (MDM) steps in. Also known as a golden record, master data is a key organizational data asset that contains the most up-to-date and accurate information for day-to-day business operations. It supports transactional, non-transactional or analytical data and is usually shared across departments to help personnel conduct analytics and make decisions around service, sales, marketing and other areas. Master data serves as the source of common data and often includes application-specific metadata and corporate dimensional data (that directly participates in transactions like a customer ID or department ID), although the data type can vary depending on the organization and its needs. Broadly, master data has to have three key qualities – less volatility, more complexity and mission-critical. Plus, it should be used and reused over and over again. A good example is contact numbers collected from customers. This kind of data is less volatile, mission-critical and can be very complex, meaning a slight error in numbers can result in a missed opportunity for the business. The ongoing practice of creating and managing the master dataset for all critical information is called master data management (MDM). It encompasses all of the technologies, processes and people that help organizations consolidate, cleanse, de-duplicate, organize, categorize, localize and augment master data as a single source of truth and then sync it with all business processes, applications and analytical platforms in use across the organization. When MDM is executed perfectly, the master data being disseminated is in sync, highly accurate and trustworthy.  Various domains of master data management include the following: In the early stages of MDM, each type of master data had to have its unique data store and business logic. Companies looking to optimize the performance of their employees, for instance, centered their MDM strategy only on the employee master data domain. However, these “single-domain” approaches are not as effective today because the data has become more complex and intertwined than ever. In order to extract full customer information at present, enterprises not only need customer master data (demographic, etc.) but also product-specific master data, which may signal their buying preferences. This is exactly where “multi-domain” MDM, which manages all different types of master data in one repository, comes in. According to a study from Aberdeen, companies leveraging multi-domain MDM have seen better results than those using single-domain MDM in terms of completeness and accuracy of data.  Efficient master data management gives an organization a single place to have an authoritative view of information, which in turn eliminates costly redundancies that occur when organizations rely upon multiple versions of data distributed across siloed systems. For instance, if a customer’s information has changed, the organization will update the master data with the new information and not turn towards driving sales and marketing efforts using the old data point present in other systems.  With MDM, organizations also get access to better data quality that is more current and suitable for analytics. The discipline makes the data format more consistent and uniform, which makes it more usable for various business processes and simplifies answering basic questions, such as “What services did the customers use during the last quarter?” or “Who were the top buyers during the same period?” Since MDM consolidates complete and trustworthy data from customers in one place, organizations can use it to better understand their target audience as well as their preferred connection channels. Then, using those insights, they can make personalized sell-up or cross-sell offers to the right person at the right time. This would not only drive up revenues, but also help cut down spending on unnecessary customer acquisition methods. Issues like sending emails to incorrect customers or making calls about resolved matters can also be avoided. MDM’s centralized copy of business-critical data also makes it much easier for organizations to create a backup. Backing up silos is a costly affair, but MDM simplifies the process, giving organizations an easy way to recover their data in the unanticipated case of a disaster or loss of information Enterprises can accelerate their time-to-market with MDM. In particular, product and supply chain master data management can help organizations cleanse and enrich almost every aspect of information required to set and meet product deadlines. A study from Stibo Systems reported that 58% of organizations using their MDM solution reduced their TTM from months to weeks and 36% brought it down from weeks to days. A well-executed MDM strategy, simplifying the handling of customer information, can also help organizations comply with regulators as well as privacy laws. Centralized management of customer master data would be especially useful in the case of General Data Protection Regulation (GDPR), which gives customers an option to get their data deleted or ported to another company providing the same service if needed. In order to ensure that the discovery of master data is done to perfection, you need to do the groundwork to make the entire enterprise data landscape accessible. This would include giving access permissions to the MDM team and flagging any obscure sources of data that might not be directly available.  Beyond this, a significant portion of time should also be spent on data profiling and usage patterns so that only relevant pieces of information are taken into the master data repository. Consultation with subject matter experts within the business can also come in handy here. Instead of creating and managing master data for a single domain, try to go for the multi-domain MDM approach. This will connect hidden data points across functions – from product to supply chain – and give a more holistic view of information to drive better results. After deciding to switch to MDM, it is important to make sure that all people who will be asked to use the master data are made aware of the incoming changes and why they are being made. Implementation should not come as a sudden change. In addition, the users of master data should be given sufficient time to adjust to the changes and an option to share their feedback, ask questions and identify gaps (if any). Beyond transparency, all personnel and departments getting access to the master data repository should be trained and retrained on various aspects of formatting and using the data. There should be workshops to educate the personnel on how they could leverage the master data to meet the set business goals.  Organizations should implement MDM in phases instead of going all-in at one go. Once the process is completed, project managers must continually work with management, employees and other stakeholders to discuss feedback and suggestions aimed at improving the system for better business outcomes. Simultaneously, they should measure the ROI of the entire process, starting from business case formulation to post-implementation, and regularly audit crucial aspects such as installation, configuration, data models and alert queues to avoid systemic delays when new business requirements and challenges arise. When conducting audits, there is a good chance of running into one or more data issues. Project managers need to expect this and be prepared to handle whatever comes up. This could be done by implementing a suitable process for triage, where the issue could be assessed and addressed depending on how urgent or critical it is.  If the issue is small, it could be addressed immediately. However, if it requires long manual work, then it could be pushed off until the launch. In a nutshell, organizations should have a set structure in place to address data quality and governance issues."
31,"What is data management? Definition, lifecycle and best practices","March 10, 2022 3:40 PM",https://venturebeat.com/2022/03/10/what-is-data-management-definition-lifecycle-and-best-practices/,"Data drives business — the best product architecture and sales team can’t overcome a lack of data needed to enable business leaders to make informed decisions, streamline operations, and build stronger customer relationships. IDC predicts that data will grow from 45 zettabytes in 2019 to a projected 175 zettabytes by 2025. Even the most organized enterprises will be overwhelmed and ineffective without a data management strategy.  Data management is the collection, organization, maintenance and analysis of data to produce insights that enable better decision-making and execution.  The goal of this process is to benefit from redundancy-free, accurate, and up-to-date data, and it requires a clear data management protocol that all teams and departments need to follow.  For example, in an enterprise setup, marketing engagement data may be stored using email automation, traffic analytics, and ad tech-platforms, while sales might be operating in a silo with data stored in the company content management system (CMS). In this scenario, the sales team cannot take advantage of marketing outreach to potential customers, and marketing is unaware of any leads being pursued by sales executives or the stage of customer journey and acquisition. This is where data management helps in unifying data from various platforms and teams, to present a single customer view that can help multiple departments to launch orchestrated and synchronized campaigns to achieve company goals.  Companies require different levels of data management maturity and sophistication. While companies with small customer bases can navigate customer needs comfortably with spreadsheets, larger companies require more sophisticated stages of data management maturity.  Here are three key data management maturity (DMM) stages to identify where a company currently stands in its journey: In a post-COVID world, data has become more than just an enabler as companies worldwide shifted quickly to remote operations and working. Today’s business capital, integrated knowledge power, intellectual property and much more as we learn to use data with increasing complexity and sophistication. A company’s leadership and future growth today is defined by how well they can collect, manage and analyze this data for business efficiency and growth. Below are a few reasons that makes data management critical to technical decision makers in 2022: Today data is increasingly used as business capital to accomplish growth and expansion, rather than merely using it for account maintenance or launching multichannel campaigns. A company that is data-rich and can useeffective data management to launch omnichannel and unified campaigns to target precise stages of customer journeys and experience, can indeed outcompete a more cash-rich company that has less data management maturity. For example, a data-rich company in stage 3 of DMM can more effectively launch retargeting ad-campaigns online-based on purchase intent of a given user based on site-usage, social media engagements, survey feedback, company asset downloads/engagements, etc. In fact, most data management platforms at this stage allow configurations to automatically trigger promotions/actions to an identified user, based on specific behaviors or trigger-points.  With data management protocols, employees and management teams have access to a centralized knowledge pool. Be it formulating the most personalized sales pitch, making customer engagement decisions or propelling ad-tech, data management is key to delivering integrated knowledge to maximize results.  Without data management methods in place, data accessed by employees and management teams will be drawn from silo-ed data storage and can result in fractured strategies and outcomes. Data management platforms sync live data from employee-inputs and other third-party tool integrations, to produce and feed the most up-to-date information. Organizations can provide role-based access to employees, who can then access information with ease.  Data management is more than just storage and access. In fact, analytics is the key reason most businesses invest in a data management tool. After all, what would be the purpose of data management if it could not be used to achieve the organization’s goals?  Data management platforms connect with third-party systems via two-way API links, and from there on it can analyze and feed the unified data back into these systems for accurate execution of sales, marketing, HR and other   business goals. These systems, in turn, feed any data collected/updated back into the DMP for final analysis of results.  For example, employee data can be fetched by an HRMS for employee surveys and then feeds the employee survey results back into the DMP for better analysis of results.  Data management lifecycle is defined as the stages that data goes through from collection to archival or deletion.  Let us now look at the various data lifecycle stages and their significance: Before any data can be managed, it must first be accumulated. This can either happen through third-party purchase or feeding data the organization has already collected.  Another way to accumulate data is through third-party integration. These can be customer management systems (CMS), marketing automation platforms, lead generation platforms, human resources management systems (HRMS) etc.  Once the data from various sources have been centrally synced and fed into the data management platform (DMP), the next step for the system is to organize and store the data. Today, in most data management DMPs, this step is done by the system with the least human input.  The core requirement from any DMP is the ability to unify and analyze the data, and serve the output in the required format to enable interpretation and decision-making.  Based on the user-query, the DMP analyzes unstructured, structured andsemi-structured data from its centralized storage to deliver the structured data in the form of spreadsheets, graphs, charts etc., or to feed data back into a thirdparty platform for further execution.  For example, a sales team executive may fetch the most updated interaction and order-book for a specific client to pitch for an upsell. In this process, the CMS may be the user interface and querying platform, which then sends the query to the DMP for processing via an API link. The data is then fetched, analyzed and sent back to the CMS.  The DMP may also have its own user interface if one wishes to skip access from a third-party party system.  Post any data input,update, or removal, data management requires a refresh of all data in the system to accommodate, organize, remove-redundancies and store the new information. Data hygiene and maintenance is needed at all stages of data interaction, which leads to addition or removal of data.  In this last stage of data management lifecycle, any data with time will be sent either to archive or is deemed unfit for any further storage and hence deleted.  Here are seven best practices to help you kick-start your data management journey in 2022: The first step to any data management strategy is to identify your organization’s current DMM stage. The plan must consider the various teams and data practices involved, quality of data hygiene and the platforms being used. If you plan to purchase a data management platform, ensuring integration of data with all third-party platforms in use is a must, or to plan for platform-migration to fit the new software.  If a company is still in stage 1, efforts need to be made to set the company-wide data-governance policy and ensure adherence to advance into stage 2. Data strategy is merely on paper until there is adherence to the principles, and adherence is a result of culture. If the data strategy is to succeed, one must cultivate a culture of data-centralization. This may include steps to ensure that all APIs are set up, employees are updating new data that cannot yet be captured automatically, that employees have the intended access-clearance for the right platforms etc. Enterprise data management operates with the objective to unify all data across all departments and platforms. This includes the company’s own operational data (legal and accounting), employee data (HR) and customer data (marketing and sales). Based on a company’s DMM stage and goals, effort can be directed to plug-in and pool data from all departments into a centralized platform like a DMP, or purely to unify customer-facing data, using a customer data platform (CDP) for instance. One of the key best practices for data management and collaboration, is to ensure that each file and document carries a description defining what it is and meant to do. Even more granular descriptions may be needed to identify each data-set and label them with contextual description. The goal is to enable understanding and use of any data by employees and management teams, who may or may not have the appropriate context for the data at the time of access. A data management policy is to enable better achievement of company goals. In other words, data policies cannot guide company goals, instead company goals need to guide how much data management sophistication is needed to achieve the goals.  For example, a company’s revenue growth targets may be decided keeping in mind various factors such as  budget for tech-stack, product inventory room, capital for expansion, existing and future liabilities, etc. To reach this goal, a company may not need a full throttle data management platform (stage 3 DMM) or have the money to deploy one. The data policy at this stage might therefore be to operate with full efficiency at stage 2 data management maturity, until the customer base or revenue targets reach a stage where additional tech-stack investments can be justified. With increasing collection and storage of data comes the need to secure it. Data security is not only needed to protect company assets, but also to fulfill assurances of customer’s data security. While financial institutions and banks require the highest possible level of data security sophistication, other sector enterprises also must ensure that data being managed by them are secure and hack-free. Based on the level of threat and type of data being handled, an enterprise data security tech-stack may include fraud detection, vulnerability management, threat identification and resolution, access management and a disaster recovery plan (DRP). For companies that are aiming to move from stage 2 of DMM to stage 3, a quality data management platform holds the key. To clarify, an optimal DMP platform does not mean the most expensive or feature-rich software, but rather the one that best fits a company’s needs.  For instance, based on geographical privacy laws, existing data-capture tech or nature of business, a personal-identity resolution feature in a DMP may not be implementable and hence not needed. For a company like this, what may appeal more are feature comparisons for judging effectiveness of campaign delivery, speed of data update, quality and depth of analytics, access-control, etc."
32,"What is customer data management (CDM)? Definition, process and strategy best practices","March 10, 2022 3:40 PM",https://venturebeat.com/2022/03/10/what-is-customer-data-management-cdm-definition-process-and-strategy-best-practices/,"Customer data management is defined as an enterprise process by which customer data is gathered, stored, updated, accessed and analyzed.  Customer interaction with brands can come from multiple channels across online and offline modes. These include store visits, social media interaction, website visits, digital asset downloads, targeted ad responses, survey responses and so on. Companies that intend to stay ahead in customer-centricity, also intend to unify this data from multiple touch-points to create a single customer view with the most up-to-date information on the latest points of interactions.  The end goal of customer data management is to ensure that the data that is used by customer-facing teams, be it marketing, sales or account management, is the most updated, well-structured and redundancy-free for maximum benefits. This is the same data that is also fed into marketing automation platforms and other customer-facing software that have a direct impact on lead generation, nurturing quality and closure-time; in other words, company revenue.  Moreover, at a time when customer interaction with brands is increasingly digital and omnichannel, the biggest challenge for brands is data hygiene. According to Forbes, in 2019, only 23% of businesses said they could reliably depend on CRM data for important decision making. With increased digitization since the COVID-19 lockdowns, companies that caught up with subpar customer data management practices saw their growth hampered by data redundancies or an inability to capture and unify all interactions online and offline. On the flip side of this coin, companies who had already executed data management principles, were at a huge advantage and saw their balance sheets expand rather than shrink. Data management today in 2022 is a cornerstone for both growth, and disaster management and recovery.  Customer data management can be explained as a three-step process to reach the final goal where data is no longer stored in silos and is unified for the company and its employees to launch orchestrated customer-facing campaigns and experiences. Here are the 3 key steps: The first step to manage any data efficiently is to ensure that its entry points are digitized and automated for the most error-free data entry. These points of entry could range from online forms filled on your website to physical store purchases. The goal is to ensure that all possible points of brand or enterprise interaction are digitized and accurately captured. The data captured across any interaction with the customer needs to be fed into a storage software, which is typically a CRM, survey software or marketing platform. At this step, data is stored and updated but still siloed and not centralized. Therefore, the data updated is only in the specific platform and not updated across all platforms and departments. This is the step for unification of data for complete customer data management. The connection can be done using API links to communicate across apps, or even better done using a customer data platform or data management platform for advanced analysis.  There are several factors that affect the quality and end result of customer data management. Some of these key aspects are: The stage of digitization is a single biggest factor affecting a company’s ability to capture, update and store customer information. For instance, in a simple scenario of a brand operating a brick-and-mortar store, are product sales captured using bar-codes or is input manually? Is customer experience survey manual or online? Is the survey sent automatically  based of interaction triggers or needs to be manually sent by staff?  By a thumb rule, the more the level of digitization and automation, the less chances of any human error occurrence and more accurate is the data capture and updates. It is not so much about staff discipline, so much as it is ensuring that human resources are focused on decision making that improve customer experience, while software and automation can take over repetitive tasks that can be done more efficiently and in an error-free manner. The captured data from customer interactions have a single goal- to unify and provide the best data input for marketing and sales operations. The whole process of capturing and feeding data into another platform is typically done using a customer data platform (CDP) or a data management platform (DMP) if a company can afford one, else it can also be done using app data exchange through APIs. In either case, what is critical is the sophistication of sales and marketing technology.  In other words, data captured cannot be efficiently utilized without having the right set of technology tools that can absorb and implement it in marketing campaigns. A typical example of a sales and marketing automation tool will be Hubspot. For email marketing specifically, be it Hubspot or MailChimp or any similar platform will need demographic data to either be uploaded or fed through another app such as a CRM, via API links. Only then can the email automation platform take over email deliveries, capture and analyze response data and feed it back into another system. A business can have several modes of interaction with customers. The number of touchpoints that have been configured for data capture and the depth of the data captured are key factors for the quality of customer data viewed and analyzed by your teams and other systems. Missing information from customer interactions lead to incomplete views of a customer’s stage of buying journey or post-purchase experience. It is therefore critical to identify all possible customer touchpoints during the early stage of customer journey mapping and make it a map for customer experience planning. This will make it easier to digitize and start capturing data from these missing links. Ofcourse, a business will always keep the cost-to-benefit ratio in mind during any investment, including digitization. But if Covid lockdowns have pushed businesses in any direction- it is digitization. Technology directors and decision makers need to know when customer data management (CDM) is needed to bring about customer experience, marketing and sales campaign effectiveness.  Below are some of the key indicators for enterprises that they need to employ CDM practices: Business-to-consumer companies have large data volumes vertically and business-to-business companies have deep data volumes horizontally. So whether it is a B2B or B2C enterprise, after a certain stage in their expansion, the customer data is no longer error-free or updated fast enough using human resources and a company employs software to capture the data.  However, the data is still typically kept in silos of platforms and departments and no process yet exists to unify the data. This leads to fractured marketing and sales campaigns and therefore, fractured results. This is when  the company is ripe for a customer data management process to unify the siloed data for the most comprehensive and updated customer views. According to a 2021 BCG report, companies that can deliver personalized customer campaigns can not only increase sales by 6% to 10%, continued personalization post-sales can also lead to longer customer lifecycles and better value per customer through increased upsells and renewals. However, personalization happens in stages. A simple email which carries the name of the reader is one level of personalization, but ensuring that even the ads and articles served to customers on company websites are based on their needs and stage of customer journey, is a much more upgraded level of personalization. The quality, accuracy and depth of personalization depends largely upon data management practices of the company. The level of sophistication of customer data management will determine the quality and depth of data captured, updated and stored, and this data is then used to deliver personalization.  If personalized experiences is the depth of a customer campaign, omnichannel experiences is the width. In other words, technology managers and directors need to ensure that customer experience, marketing and sales teams are equipped with the right data and technology to be able to deliver uniform experiences invariant of channel or platform of interaction with a customer. This is a shift from multi-channel campaigns where all channels are utilized but may not be uniform due to siloed data that exists in these channels and platforms.  Customer data management needs to now be employed to unify this data and finally enable teams to execute omnichannel customer campaigns. Enterprises today need to move past only data-centricity or only customer-centricity, and need to combine the two into customer-centricity that is data-centric. This means technology directors and customer-facing teams have to work together for data governance that is based on a customer-centric culture and aims to utilize the data captured for enhanced customer experiences across all channels, platforms and points of interactions.  A CDM strategy that includes a customer-centric culture ensures that all data is utilized with the aim to improve customer satisfaction, while data-centricity ensures that the data is captured from all interactions and is as fresh as technologically possible at their current levels of tech-maturity. Capturing customer data to orchestrate meaningful campaigns that enhance experiences is a great benefit for any enterprise and gains the company a good reputation and demand. However, a single slip in data security can undo all these benefits and can bring about negative attention from the media and law enforcement agencies.  With data capture on the rise, so are data breaches. It is the responsibility of the data security team to ensure that all customer data is captured and secured using encryption and/or security software. A data governance policy is a company-wide set of rules that govern how data is captured, where it is stored, and who can access and update it. Data governance rules are meant for all departments across the enterprise, and encompasses the governance of internal company data and external customer data. The goal of such a policy is to ensure compliance in data management across the enterprise to minimize risks of data loss, corruption, breaches and redundancies, while maximizing data capture, unification and utility.  An example of a common mistake and loophole in enterprise data management is employees purchasing customer-related software for a specific need and keeping it siloed. Often only a few people may be using this software, but the data captured in this software can be valuable across customer-facing departments and teams. This is typically the case, when data governance is weak or shallow and does not entail how a new software needs integration with existing platforms.  Ensuring data integration across all and new customer-facing software, preferably as part of the data governance policy itself, is key to ensure that any new customer-facing platform is immediately piped into the larger customer data management strategy. Data orchestration platforms such as Data Management Platforms (DMP) and Customer Data Platforms (CDP) are used to manage complex, omnichannel and highly personalized campaigns to manage customer experience and/ customer acquisition campaigns. This includes syncing web-content recommendation engines, email campaign platforms, adtech platforms, CRM software etc., and delivering campaigns based on triggers and live data from across 3rd party or in-house software."
33,Databricks targets healthcare with industry-specific lakehouse,"March 9, 2022 8:07 AM",https://venturebeat.com/2022/03/09/databricks-targets-healthcare-with-industry-specific-lakehouse/,"San Francisco-headquartered data and AI company Databricks today expanded its product portfolio with another vertical-specific platform – Lakehouse for Healthcare and Life Sciences. The offering, as the company explained, brings a set of tailored data and AI solutions aimed at solving the most common challenges enterprises face while innovating in the healthcare industry. It follows the launch of Databricks’ lakehouse for retail and financial services sectors. “As organizations fully transition to electronic medical records, new data types like genomics evolve, and IoT and wearables take off, the industry is awash in massive amounts of data. But…teams don’t have the tools to properly use it,” Michael Hartman, senior vice president of regulated industries at Databricks, said. “With Lakehouse for Healthcare and Life Sciences, we can drive transformation across the entire healthcare ecosystem and help empower our customers to solve specific industry challenges and drive better outcomes for the future of healthcare.” Currently, enterprises in the healthcare sector largely rely on legacy data architectures that keep information from different systems and functions in fragmented silos. This makes advanced analytics difficult, restricting the pace of innovation across areas such as patient care and drug discovery. The vertical-specific lakehouse, on the other hand, eliminates this challenge by providing health institutions with a single cloud-backed platform for data management, analytics and advanced AI. It enables organizations to leverage data easily and accelerate the development of more advanced, data-driven solutions such as those aimed at disease risk prediction, medical image classification, biomarker discovery and drug development. Cross-functional teams – from physician-scientists to computational biologists – can also use data from this product to build a holistic view of the patient and make real-time decisions. To make these applications simpler, Lakehouse for Healthcare comes with a bunch of solution accelerators and open-source libraries (such as GLOW for genomics) backed by a certified ecosystem of partners. When combined together, these elements help data users jumpstart their analytics projects and save weeks to months of development time. The partner ecosystem, Databricks explained, includes companies like Deloitte, Lovelytics, John Snow Labs and ZS Associates. John Snow Labs will help enterprises analyze unstructured medical text using NLP for use cases such as oncology research, drug safety monitoring and anonymizing personal health information. Meanwhile, Lovelytics and ZS Associates help with automating the ingestion of streaming Fast Healthcare Interoperability Resource bundles and improving biomarker discovery for precision medicine.  Databricks is making the new lakehouse offering generally available starting today. However, some enterprises have already had a chance to use it early, including GE Healthcare, Regeneron, ThermoFisher and Walgreens. “The Databricks Lakehouse for Healthcare and Life Sciences is helping GE Healthcare with a modern, open and collaborative platform to build patient views across care pathways. By unifying our data in a single platform with a full suite of analytics and ML capabilities, we’ve diminished costly legacy data silos and equipped our teams with timely and accurate insights.” Joji George, CTO of LCS Digital at GE Healthcare, said. The move further expands Databricks’ lakehouse ecosystem, which competes directly with Snowflake’s data cloud. Other players in the same space are Dremio, Google BigQuery, and Amazon Redshift."
34,Fivetran improves data transformation with integrated scheduling,"January 18, 2022 6:30 AM",https://venturebeat.com/2022/01/18/fivetran-improves-data-transformation-with-integrated-scheduling/,"Fivetran, a data integration platform that helps enterprises “extract, load, and transform” (ELT) data from multiple SaaS and on-prem sources, today announced two key improvements to Fivetran Transformation for dbt Core – integrated scheduling and data lineage graphs. Transformations are critical in ELT as they turn raw data into clean datasets for use in downstream analytics workflows — from basic reporting to data science. Over a year ago, to simplify this transformation process and allow faster access to analytics-ready data, Fivetran integrated with Dbt Labs’ open-source transformation framework dbt Core and announced the beta launch of Fivetran Transformations for dbt Core. The offering, it said, will allow customers to take advantage of an automated cloud data integration experience in a single environment, complete with cleaning, testing, transformation, modeling, and documentation of datasets. Now, with the launch of new features, Fivetran is improving this service and deepening its integration with dbt Core. Integrated scheduling, according to the company, will allow its users to schedule their dbt Core models to run automatically following the completion of a Fivetran connector sync. This will reduce data latency and speed up the end-to-end ELT pipeline while also helping customers save money on unnecessary compute costs by only running transformations on new or updated data. Data lineage graphs, on the other hand, will visualize dbt Core data models in Fivetran, allowing users to better track and manage their end-to-end data pipelines in support of data governance. This particular feature will eliminate the need for data analysts to comb through SQL code to determine relationships between models, and provide a visual representation that data engineers can use to map data movement throughout the transformation process. Moreover, these graphs could also be shared with data analysts and other business users across the organization for a more collaborative experience, the company notes. “Fivetran’s ability to orchestrate dbt Core models brings the E, L, and T together and eliminates previous gaps in the process,” a Fivetran spokesperson said. “Our users are clear about their need for the freshest data while controlling their transformation costs. With Fivetran Transformations, our complete ELT data pipelines empower our customers to make revenue-impacting, data-driven decisions.” The new capabilities come just a few months after Fivetran’s mega financing round of $565 million at a $5.6 billion valuation. The Oakland, California-based company had also acquired data replication startup HVR to provide “modern analytics for the world’s most business-critical data without compromising security, performance, or ease of use.” The terms of the deal were not disclosed."
35,Trifacta expands data preparation tools with Databricks integration,"April 7, 2021 6:00 AM",https://venturebeat.com/2021/04/07/trifacta-expands-data-preparation-tools-with-databricks-integration/,"Trifacta today announced it has integrated its data preparation tools with a data warehouse platform based on the open source Apache Spark framework provided by Databricks. This is in addition to repositories based on an open source data built tool (DBT) that is maintained by Fishtown Analytics. In both cases, Trifacta is extending the reach of tools it provides for managing data pipelines to platforms that are widely employed in the cloud to process and analyze data, Trifacta CEO Adam Wilson said. Trifacta traces its lineage back to a research project that involved professors from Stanford University and the University of California at Berkley and resulted in a visual tool that enables data analysts without programming skills to load data. In effect, Trifacta automated extract, transform, and load (ETL) processes that had previously required an IT specialist to perform. There is no shortage of visual tools that let end users without programming skills migrate data. But Trifacta has extended its offerings to a platform that enables organizations to manage the data pipeline process on an end-to-end basis as part of its effort to meld data operations (DataOps) with machine learning operations (MLOps). The goal is to enable data analysts to self-service their own data requirements without requiring any intervention on the part of an IT team, Wilson noted. Google and IBM already resell the Trifacta data preparation platform, and the company has established alliances with both Amazon Web Services (AWS) and Microsoft. Those relationships enable organizations to employ Trifacta as a central hub for moving data in and out of cloud platforms. The alliance with Databricks and the support for DBT further extend those capabilities at a time when organizations have begun to more routinely employ multiple cloud frameworks to process and analyze data, Wilson said. In general, data engineering has evolved into a distinct IT discipline because of the massive amount of data that needs to be moved and transformed. While visual tools make it possible for data analysts to self-service their own data requirements, organizations are now also looking to programmatically move data to clouds as part of a larger workflow. Many individuals that have ETL programming expertise, often referred to as data engineers, are now in even higher demand than data analysts, Wilson said. Once considered the IT equivalent of a janitorial task that revolved mainly around backup and recovery tasks, data engineering is now the discipline around which all large-scale data science projects revolve, Wilson noted. In fact, IT professionals with ETL skills have reinvented themselves to become data engineers, Wilson added. “In the last 12 months, data engineering has become the hottest job in all of IT,” Wilson said. It remains to be seen just how automated data engineering processes can become in the months and years ahead. Not only is there more data to be processed and analyzed than ever, the types of data that need to be processed have never been more varied. Going forward, a larger percentage of data will be processed and analyzed on edge computing platforms, where it is created and consumed. But the aggregated results of all that data processing will still need to be shared with multiple data warehouse platforms residing in the cloud and in on-premises IT environments. Regardless of where data is processed, the sheer volume of data moving across the extended enterprise will continue to increase exponentially. The issue now is figuring out how to automate the movement of that data in a way that scales much more easily."
